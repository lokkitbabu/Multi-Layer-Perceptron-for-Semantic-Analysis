paper_id,methods,model1,model2,model3,model4,model5,model6
on-enhancing-speech-emotion-recognition-using,"GAN, Convolution",GAN,,,,,
snap-ml-a-hierarchical-framework-for-machine,Logistic Regression,Logistic Regression,,,,,
a-novel-hybrid-machine-learning-model-for,SVM,SVM,,,,,
laplacian-smoothing-gradient-descent,Logistic Regression,Logistic Regression,,,,,
examining-the-use-of-neural-networks-for,SVM,SVM,,,,,
genesis-of-basic-and-multi-layer-echo-state,AutoEncoder,Autoencoder,,,,,
going-deeper-in-spiking-neural-networks-vgg,"Dense Connections, Convolution, Softmax, VGG, Max Pooling, ReLU, Dropout",VGG,,,,,
machine-learning-cicy-threefolds,SVM,SVM,,,,,
q-space-novelty-detection-with-variational,VAE,VAE,,,,,
radialgan-leveraging-multiple-datasets-to,"GAN, Convolution",GAN,,,,,
pros-and-cons-of-gan-evaluation-measures,"GAN, Convolution",GAN,,,,,
k-beam-minimax-efficient-optimization-for,"SPEED, GAN, Convolution",GAN,,,,,
deep-learning-based-inverse-method-for-layout,"AutoEncoder, VAE",VAE,Autoencoder,,,,
conditional-linear-regression,Linear Regression,Linear Regression,,,,,
adversarial-regression-with-multiple-learners,Linear Regression,Linear Regression,,,,,
killing-four-birds-with-one-gaussian-process,Gaussian Process,Gaussian Process,,,,,
reverse-iterative-volume-sampling-for-linear,Linear Regression,Linear Regression,,,,,
a-machine-learning-framework-for-stock,Logistic Regression,Logistic Regression,,,,,
semi-supervised-clustering-with-neural,"AutoEncoder, Convolution",Autoencoder,,,,,
graph-based-regularization-for-regression,Linear Regression,Linear Regression,,,,,
autoencoders-learn-generative-linear-models,AutoEncoder,Autoencoder,,,,,
nest-a-neural-network-synthesis-tool-based-on,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout, Grouped Convolution, Pruning",AlexNet,,,,,
adversarial-attacks-on-face-detectors-using,"RPN, RoIPool, Convolution, Softmax, Faster R-CNN",CNN,Faster R-CNN,,,,
recurrent-deep-embedding-networks-for,"AutoEncoder, SHAP",Autoencoder,,,,,
mpdcompress-matrix-permutation-decomposition,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, LeNet, Max Pooling, 1x1 Convolution, ReLU, Dropout, Grouped Convolution, Pruning",LeNet,AlexNet,,,,
neural-discrete-representation-learning,"Batch Normalization, Residual Connection, VQ-VAE, Dilated Causal Convolution, Convolution, Residual Block, ReLU, Adam",VAE,,,,,
autozoom-autoencoder-based-zeroth-order,AutoEncoder,Autoencoder,,,,,
deep-reinforcement-learning-in-ice-hockey-for,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
splitting-source-code-identifiers-using,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
detecting-deceptive-reviews-using-generative,"GAN, Convolution",GAN,,,,,
graph2seq-graph-to-sequence-learning-with,"Tanh Activation, LSTM, Sigmoid Activation, Seq2Seq",Seq2Seq,LSTM,,,,
collective-online-learning-of-gaussian,Gaussian Process,Gaussian Process,,,,,
ecornn-efficient-computing-of-lstm-rnn,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
meta-learning-with-differentiable-closed-form,Logistic Regression,Logistic Regression,,,,,
gsae-an-autoencoder-with-embedded-gene-set,AutoEncoder,Autoencoder,,,,,
edge-attention-based-multi-relational-graph,GCN,GCN,,,,,
ntmaldetect-a-machine-learning-approach-to,SVM,SVM,,,,,
chief-complaint-classification-with-recurrent,"GRU, SPEED",GRU,,,,,
number-sequence-prediction-problems-for,"Tanh Activation, GRU, LSTM, Sigmoid Activation",GRU,LSTM,,,,
approximating-the-void-learning-stochastic,"GAN, Convolution",GAN,,,,,
towards-explaining-anomalies-a-deep-taylor,SVM,SVM,,,,,
distribution-based-label-space-transformation,Logistic Regression,Logistic Regression,,,,,
automated-design-of-collective-variables,Logistic Regression,Logistic Regression,,,,,
towards-autonomous-reinforcement-learning,Gaussian Process,Gaussian Process,,,,,
classification-of-protein-crystallization-x,"ResNet, Average Pooling, Dense Connections, Global Average Pooling, VGG, Residual Connection, Bottleneck Residual Block, Convolution, Residual Block, Softmax, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Dropout, Batch Normalization",VGG,ResNet,,,,
towards-budget-driven-hardware-optimization,DCNN,CNN,DCNN,,,,
a-cost-sensitive-deep-belief-network-for,Deep Belief Network,Deep Belief Network,,,,,
evolving-mario-levels-in-the-latent-space-of,"GAN, Convolution",GAN,,,,,
an-anti-fraud-system-for-car-insurance-claim,"Dense Connections, SPEED, Convolution, Softmax, VGG, Max Pooling, ReLU, Dropout",VGG,,,,,
a-multi-state-diagnosis-and-prognosis,Deep Belief Network,Deep Belief Network,,,,,
machine-learning-for-exam-triage,"Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, CheXNet, DenseNet, Convolution, Softmax, Max Pooling, 1x1 Convolution, Dense Block, ReLU, Dropout, Kaiming Initialization, Batch Normalization",CheXNet,DenseNet,,,,
efficiently-learning-nonstationary-gaussian,Gaussian Process,Gaussian Process,,,,,
removing-confounding-factors-associated,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
smart-surveillance-as-an-edge-network-service,"Convolution, SVM",SVM,,,,,
plusemo2vec-at-semeval-2018-task-1-exploiting,Logistic Regression,Logistic Regression,,,,,
a-robot-to-shape-your-natural-plant-the,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
modeling-long-and-short-term-temporal,"Gaussian Process, Convolution",Gaussian Process,,,,,
a-support-tensor-train-machine,SVM,SVM,,,,,
melanogans-high-resolution-skin-lesion,"Leaky ReLU, DCGAN, LAPGAN, GAN, Convolution, Laplacian Pyramid, ReLU, Batch Normalization",GAN,DCGAN,,,,
data-augmentation-by-pairing-samples-for,"GoogLeNet, Average Pooling, Dense Connections, Inception Module, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout, Auxiliary Classifier",GoogleNet,LeNet,,,,
hyperspherical-variational-auto-encoders,VAE,VAE,,,,,
high-quality-nonparallel-voice-conversion,"Cycle Consistency Loss, GAN Least Squares Loss, ReLU, Batch Normalization, Tanh Activation, Residual Connection, CycleGAN, Sigmoid Activation, Leaky ReLU, PatchGAN, Instance Normalization, Convolution, Residual Block",GAN,,,,,
a-sentiment-and-semantics-based-approach-for,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
performance-evaluation-and-hyperparameter,"Logistic Regression, GAM",Logistic Regression,,,,,
ai-blue-book-vehicle-price-prediction-using,Linear Regression,Linear Regression,,,,,
logistic-regression-the-importance-of-being,Logistic Regression,Logistic Regression,,,,,
learning-based-dequantization-for-image,DCNN,CNN,DCNN,,,,
stacked-neural-networks-for-end-to-end,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
forecasting-economics-and-financial-time,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
variational-zero-inflated-gaussian-processes,Gaussian Process,Gaussian Process,,,,,
a-neural-network-architecture-combining-gated,"GRU, Softmax",GRU,,,,,
fast-cosmic-web-simulations-with-generative,"GAN, Convolution",GAN,,,,,
parlai-a-dialog-research-software-platform,"Tanh Activation, LSTM, Sigmoid Activation, Seq2Seq",Seq2Seq,LSTM,,,,
on-breast-cancer-detection-an-application-of,"Linear Regression, Softmax",Linear Regression,,,,,
machine-learning-in-downlink-coordinated,SVM,SVM,,,,,
the-history-began-from-alexnet-a,Deep Belief Network,Deep Belief Network,,,,,
missing-data-in-sparse-transition-matrix,Linear Regression,Linear Regression,,,,,
dimensionally-tight-bounds-for-second-order,Logistic Regression,Logistic Regression,,,,,
autoprognosis-automated-clinical-prognostic,Gaussian Process,Gaussian Process,,,,,
a-machine-learning-approach-to-air-traffic,Logistic Regression,Logistic Regression,,,,,
restricted-eigenvalue-from-stable-rank-with,"Linear Regression, Normalizing Flows, Affine Coupling",Linear Regression,,,,,
global-scale-phylogenetic-linguistic,SVM,SVM,,,,,
generic-black-box-end-to-end-attack-against,SVM,SVM,,,,,
a-machine-learning-approach-for-virtual-flow,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
bridge-type-classification-supervised,SVM,SVM,,,,,
deep-versus-wide-convolutional-neural,DCNN,CNN,DCNN,,,,
directly-and-efficiently-optimizing,Logistic Regression,Logistic Regression,,,,,
scalable-meta-learning-for-bayesian,Gaussian Process,Gaussian Process,,,,,
task-aware-compressed-sensing-with-generative,"GAN, Convolution",GAN,,,,,
memory-fusion-network-for-multi-view,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
stochastic-primal-dual-proximal-extragradient,Logistic Regression,Logistic Regression,,,,,
a-gaussian-process-regression-model-for,Gaussian Process,Gaussian Process,,,,,
siamese-neural-networks-with-random-forest,GRU,GRU,,,,,
hybrid-gradient-boosting-trees-and-neural,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
how-generative-adversarial-networks-and-their,"AutoEncoder, GAN, Convolution",GAN,Autoencoder,,,,
shadho-massively-scalable-hardware-aware,"U-Net, Concatenated Skip Connection, Convolution, Max Pooling, SVM, ReLU",SVM,,,,,
deeper-insights-into-graph-convolutional,"GCN, Convolution",GCN,,,,,
mr-image-reconstruction-using-deep-density,VAE,VAE,,,,,
fusion-of-ann-and-svm-classifiers-for-network,SVM,SVM,,,,,
cortical-microcircuits-as-gated-recurrent,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
implementation-of-deep-convolutional-neural,"Gaussian Process, Softmax, Adam, ReLU",Gaussian Process,,,,,
a-deep-belief-network-based-machine-learning,Deep Belief Network,Deep Belief Network,,,,,
improved-inception-residual-convolutional,"Inception-v4, DCNN, Average Pooling, Reduction-B, Reduction-A, Inception-A, Convolution, Softmax, Inception-C, Max Pooling, 1x1 Convolution, Inception-B, Dropout",CNN,DCNN,,,,
an-empirical-evaluation-for-the-intrusion,SVM,SVM,,,,,
network-scale-traffic-modeling-and,Gaussian Process,Gaussian Process,,,,,
feature-extraction-for-machine-learning-based,SVM,SVM,,,,,
cross-validation-with-confidence,Linear Regression,Linear Regression,,,,,
adaptive-regularization-for-lasso-models-in,Linear Regression,Linear Regression,,,,,
logo-synthesis-and-manipulation-with,"GAN, Convolution",GAN,,,,,
multiple-adaptive-bayesian-linear-regression,Linear Regression,Linear Regression,,,,,
manifold-valued-image-generation-with,"WGAN, Convolution",GAN,,,,,
towards-personalized-modeling-of-the-female,Gaussian Process,Gaussian Process,,,,,
wisdom-of-the-crowd-from-unsupervised,Linear Regression,Linear Regression,,,,,
practical-hash-functions-for-similarity,SVM,SVM,,,,,
training-large-margin-host-pathogen-protein,SVM,SVM,,,,,
proximal-gradient-method-with-extrapolation,Logistic Regression,Logistic Regression,,,,,
squishednets-squishing-squeezenet-further-for,"Global Average Pooling, Softmax, Convolution, Max Pooling, 1x1 Convolution, Fire Module, ReLU, Dropout, Xavier Initialization, Residual Connection, SqueezeNet, Average Pooling",SqueezeNet,,,,,
accelerating-cross-validation-in-multinomial,Logistic Regression,Logistic Regression,,,,,
a-sequence-based-mesh-classifier-for-the,"SVM, Discrete Cosine Transform",SVM,,,,,
data-fusion-and-machine-learning-integration,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
intelligent-fault-analysis-in-electrical,"Tanh Activation, LSTM, Sigmoid Activation, SVM",SVM,LSTM,,,,
latentpoison-adversarial-attacks-on-the,AutoEncoder,Autoencoder,,,,,
performance-comparison-of-intrusion-detection,"Firefly algorithm, SPEED, SVM",SVM,,,,,
gaussian-kernel-in-quantum-paradigm,SVM,SVM,,,,,
improve-sat-solving-with-machine-learning,Logistic Regression,Logistic Regression,,,,,
auto-differentiating-linear-algebra,"Gaussian Process, Linear Regression",Linear Regression,Gaussian Process,,,,
contextual-regression-an-accurate-and,SVM,SVM,,,,,
regularization-approaches-for-support-vector,SVM,SVM,,,,,
exploring-the-use-of-text-classification-in,SVM,SVM,,,,,
calibration-of-machine-learning-classifiers,Logistic Regression,Logistic Regression,,,,,
deep-health-care-text-classification,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
distributed-statistical-machine-learning-in,Linear Regression,Linear Regression,,,,,
correcting-boundary-over-exploration,Gaussian Process,Gaussian Process,,,,,
causal-rule-sets-for-identifying-subgroups,Logistic Regression,Logistic Regression,,,,,
clickbait-detection-using-word-embeddings,Linear Regression,Linear Regression,,,,,
machine-learning-for-drug-overdose,Gaussian Process,Gaussian Process,,,,,
generating-nontrivial-melodies-for-music-as-a,AutoEncoder,Autoencoder,,,,,
iterative-machine-learning-for-precision,Gaussian Process,Gaussian Process,,,,,
lstm-a-search-space-odyssey,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
remote-sensing-image-classification-with,Gaussian Process,Gaussian Process,,,,,
supervised-q-walk-for-learning-vector,Q-Learning,Q-Learning,,,,,
facial-key-points-detection-using-deep,"Dense Connections, Convolution, LeNet",LeNet,,,,,
convergence-analysis-of-distributed,"Logistic Regression, SGD",Logistic Regression,,,,,
a-deep-learning-model-for-traffic-flow-state,"SPEED, Deep Belief Network",Deep Belief Network,,,,,
anomaly-detection-for-a-water-treatment,SVM,SVM,,,,,
unsupervised-domain-adaptation-for-robust,AutoEncoder,Autoencoder,,,,,
quantum-autoencoders-via-quantum-adders-with,AutoEncoder,Autoencoder,,,,,
yet-another-adni-machine-learning-paper,SVM,SVM,,,,,
identifying-restaurant-features-via-sentiment,SVM,SVM,,,,,
deep-models-under-the-gan-information-leakage,"GAN, Convolution",GAN,,,,,
an-unsupervised-long-short-term-memory-neural,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
source-localization-in-an-ocean-waveguide,SVM,SVM,,,,,
arigan-synthetic-arabidopsis-plants-using,"Leaky ReLU, DCGAN, Convolution, ReLU, Batch Normalization",GAN,DCGAN,,,,
designing-and-building-the-mlpack-open-source,Logistic Regression,Logistic Regression,,,,,
adaptive-svm-learning-with-privileged,SVM,SVM,,,,,
machine-learning-approach-for-detection-of,SVM,SVM,,,,,
causally-regularized-learning-with-agnostic,Logistic Regression,Logistic Regression,,,,,
differentially-private-ordinary-least-squares,Linear Regression,Linear Regression,,,,,
revisiting-the-effectiveness-of-off-the-shelf,"Global Average Pooling, Softmax, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Bottleneck Residual Block, Residual Connection, ResNet, Average Pooling",ResNet,,,,,
projectionnet-learning-efficient-on-device,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
revisiting-activation-regularization-for,"Dropout, Temporal Activation Regularization, Recurrent Dropout, Tanh Activation, LSTM, Sigmoid Activation, Activation Regularization",LSTM,,,,,
application-of-support-vector-machine,"Logistic Regression, SVM",SVM,Logistic Regression,,,,
a-robust-multi-batch-l-bfgs-method-for,Logistic Regression,Logistic Regression,,,,,
freehand-ultrasound-image-simulation-with,"GAN, Convolution",GAN,,,,,
learning-deep-latent-spaces-for-multi-label,AutoEncoder,Autoencoder,,,,,
optimization-methods-for-supervised-machine,Logistic Regression,Logistic Regression,,,,,
recurrent-neural-networks-with-specialized,"Tanh Activation, LSTM, CRF, Sigmoid Activation",LSTM,,,,,
developing-bug-free-machine-learning-systems,AutoEncoder,Autoencoder,,,,,
multi-level-svm-based-cad-tool-for,SVM,SVM,,,,,
deep-learning-autoencoder-approach-for,AutoEncoder,Autoencoder,,,,,
towards-proof-synthesis-guided-by-neural,"Tanh Activation, LSTM, Sigmoid Activation, Seq2Seq",Seq2Seq,LSTM,,,,
asynchronous-distributed-variational-gaussian,Gaussian Process,Gaussian Process,,,,,
discriminatively-learned-hierarchical-rank,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout, Grouped Convolution",AlexNet,,,,,
efficient-modeling-of-latent-information-in,Gaussian Process,Gaussian Process,,,,,
supervised-machine-learning-for-signals,Logistic Regression,Logistic Regression,,,,,
real-time-adaptive-image-compression,AutoEncoder,Autoencoder,,,,,
a-feature-embedding-strategy-for-high-level,"DCNN, Softmax",CNN,DCNN,,,,
handwritten-bangla-digit-recognition-using,"Deep Belief Network, Dropout",Deep Belief Network,,,,,
projected-semi-stochastic-gradient-descent,SVM,SVM,,,,,
machine-learning-on-sequential-data-using-a,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
offline-handwritten-recognition-of-malayalam,SVM,SVM,,,,,
near-data-processing-for-differentiable,"Logistic Regression, SSD, SGD, Non Maximum Suppression, Convolution, 1x1 Convolution",Logistic Regression,,,,,
inception-recurrent-convolutional-neural,DCNN,CNN,DCNN,,,,
semi-supervised-classification-for-dynamic,"SVM, LDA",SVM,,,,,
fast-stochastic-variance-reduced-gradient,SVM,SVM,,,,,
joint-regression-and-ranking-for-image,Gaussian Process,Gaussian Process,,,,,
dyvedeep-dynamic-variable-effort-deep-neural,"Dense Connections, OverFeat, Softmax, Convolution, Max Pooling, VGG, ReLU, Dropout",VGG,,,,,
detecting-unseen-falls-from-wearable-devices,AutoEncoder,Autoencoder,,,,,
symbolic-regression-algorithms-with-built-in,Linear Regression,Linear Regression,,,,,
a-statistical-machine-learning-approach-to,Gaussian Process,Gaussian Process,,,,,
learning-in-implicit-generative-models,"GAN, Convolution",GAN,,,,,
trained-ternary-quantization,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout, Grouped Convolution",AlexNet,,,,,
ese-efficient-speech-recognition-engine-with,"Pruning, Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
parallel-long-short-term-memory-for-multi,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
multi-feature-classifiers-for-burst-detection,"Logistic Regression, SVM",SVM,Logistic Regression,,,,
on-sgds-failure-in-practice-characterizing,"Linear Regression, SGD",Linear Regression,,,,,
toward-the-automated-analysis-of-complex,Logistic Regression,Logistic Regression,,,,,
optimizing-cost-sensitive-svm-for-imbalanced,SVM,SVM,,,,,
the-microsoft-2016-conversational-speech,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Residual Connection, Bottleneck Residual Block, ResNet, Average Pooling",ResNet,,,,,
on-the-effectiveness-of-discretizing,Logistic Regression,Logistic Regression,,,,,
data-programming-creating-large-training-sets,"Tanh Activation, Logistic Regression, LSTM, Sigmoid Activation",Logistic Regression,LSTM,,,,
p-dla-a-predictive-system-model-for-onshore,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
what-is-relevant-in-a-text-document-an,SVM,SVM,,,,,
word-embeddings-and-recurrent-neural-networks,SVM,SVM,,,,,
a-new-recurrent-neural-network-based,Logistic Regression,Logistic Regression,,,,,
machine-learning-linear-and-bayesian-models,Logistic Regression,Logistic Regression,,,,,
predicting-brain-age-with-deep-learning-from,Gaussian Process,Gaussian Process,,,,,
a-review-of-intelligent-practices-for,Logistic Regression,Logistic Regression,,,,,
behavior-based-machine-learning-a-hybrid,SVM,SVM,,,,,
fast-and-energy-efficient-cnn-inference-on,"Global Average Pooling, Softmax, Convolution, Max Pooling, 1x1 Convolution, Fire Module, ReLU, Dropout, Xavier Initialization, Residual Connection, SqueezeNet, Average Pooling",SqueezeNet,,,,,
private-empirical-risk-minimization-beyond,Linear Regression,Linear Regression,,,,,
predicting-clinical-events-by-combining,Logistic Regression,Logistic Regression,,,,,
gradients-of-counterfactuals,"GoogLeNet, LSTM, Average Pooling, Dense Connections, Inception Module, Auxiliary Classifier, Tanh Activation, Sigmoid Activation, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout",GoogleNet,LeNet,LSTM,,,
privlogit-efficient-privacy-preserving,Logistic Regression,Logistic Regression,,,,,
extracting-actionability-from-machine,SVM,SVM,,,,,
an-improved-approach-for-prediction-of,Logistic Regression,Logistic Regression,,,,,
a-multi-task-learning-model-for-malware,"Tanh Activation, LSTM, Sigmoid Activation, Seq2Seq",Seq2Seq,LSTM,,,,
semi-supervised-active-learning-for-support,SVM,SVM,,,,,
neural-paraphrase-generation-with-stacked,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
multiple-regularizations-deep-learning-for,"SVM, Dropout",SVM,,,,,
caffeinated-fpgas-fpga-framework-for,"Dense Connections, Convolution, Softmax, VGG, Max Pooling, ReLU, Dropout",VGG,,,,,
towards-the-effectiveness-of-deep,DCNN,CNN,DCNN,,,,
asynchronous-stochastic-proximal-optimization,Logistic Regression,Logistic Regression,,,,,
gaussian-process-pseudo-likelihood-models-for,Gaussian Process,Gaussian Process,,,,,
multi-task-prediction-of-disease-onsets-from,Logistic Regression,Logistic Regression,,,,,
accelerating-deep-learning-with-shrinkage-and,"SPEED, Deep Belief Network, Convolution",Deep Belief Network,,,,,
dialogue-manager-domain-adaptation-using,Gaussian Process,Gaussian Process,,,,,
using-natural-language-processing-to-screen,SVM,SVM,,,,,
language-detection-for-short-text-messages-in,SVM,SVM,,,,,
conditional-sparse-linear-regression,Linear Regression,Linear Regression,,,,,
semi-supervised-prediction-of-gene-regulatory,SVM,SVM,,,,,
online-adaptation-of-deep-architectures-with,AutoEncoder,Autoencoder,,,,,
vdnn-virtualized-deep-neural-networks-for,"GoogLeNet, Average Pooling, Dense Connections, AlexNet, Inception Module, VGG, Grouped Convolution, Auxiliary Classifier, OverFeat, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout",GoogleNet,LeNet,VGG,AlexNet,,
understanding-and-improving-convolutional,"SGD with Momentum, CReLU, Leaky ReLU, Dense Connections, Softmax, Convolution, VGG, Max Pooling, Weight Decay, ReLU, Adam, Dropout",VGG,,,,,
error-resilient-machine-learning-in-near,SVM,SVM,,,,,
a-multilevel-framework-for-sparse,Logistic Regression,Logistic Regression,,,,,
a-learning-algorithm-for-relational-logistic,Logistic Regression,Logistic Regression,,,,,
distributed-submodular-maximization,Gaussian Process,Gaussian Process,,,,,
an-uncertain-future-forecasting-from-static,AutoEncoder,Autoencoder,,,,,
combining-multiscale-features-for,SVM,SVM,,,,,
cltorch-a-hardware-agnostic-backend-for-the,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, VGG, 1x1 Convolution, ReLU, Dropout, Grouped Convolution",VGG,AlexNet,,,,
max-margin-feature-selection,SVM,SVM,,,,,
curie-a-method-for-protecting-svm-classifier,SVM,SVM,,,,,
arock-an-algorithmic-framework-for,Logistic Regression,Logistic Regression,,,,,
support-vector-algorithms-for-optimizing-the,SVM,SVM,,,,,
web-spam-detection-using-multiple-kernels-in,SVM,SVM,,,,,
an-evaluation-of-randomized-machine-learning,"Logistic Regression, Dropout",Logistic Regression,,,,,
neurohex-a-deep-q-learning-hex-agent,Q-Learning,Q-Learning,,,,,
can-boosting-with-svm-as-week-learners-help,SVM,SVM,,,,,
developing-quantum-annealer-driven-data,Logistic Regression,Logistic Regression,,,,,
dasa-domain-adaptation-in-stacked,AutoEncoder,Autoencoder,,,,,
efficient-multiscale-gaussian-process,Gaussian Process,Gaussian Process,,,,,
cells-in-multidimensional-recurrent-neural,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
reinforcement-learning-approach-for-real-time,Q-Learning,Q-Learning,,,,,
generating-images-with-perceptual-similarity,AutoEncoder,Autoencoder,,,,,
feature-representation-for-icu-mortality,Logistic Regression,Logistic Regression,,,,,
binding-via-reconstruction-clustering,AutoEncoder,Autoencoder,,,,,
using-machine-learning-for-medium-frequency,Deep Belief Network,Deep Belief Network,,,,,
poseidon-a-system-architecture-for-efficient,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, Adam, ReLU, Dropout, Grouped Convolution",AlexNet,,,,,
a-novel-approach-to-distributed-multi-class,SVM,SVM,,,,,
predicting-the-top-and-bottom-ranks-of,SVM,SVM,,,,,
distributed-machine-learning-via-sufficient-1,Logistic Regression,Logistic Regression,,,,,
auc-maximized-deep-convolutional-neural,DCNN,CNN,DCNN,,,,
the-wilson-machine-for-image-modeling,Logistic Regression,Logistic Regression,,,,,
kernel-methods-for-accurate-uwb-based-ranging,Gaussian Process,Gaussian Process,,,,,
neutralized-empirical-risk-minimization-with,SVM,SVM,,,,,
an-empirical-study-on-sentiment,SVM,SVM,,,,,
gaussian-process-random-fields,Gaussian Process,Gaussian Process,,,,,
within-brain-classification-for-brain-tumor,SVM,SVM,,,,,
distributed-parameter-map-reduce,Logistic Regression,Logistic Regression,,,,,
convolutional-lstm-network-a-machine-learning,"Tanh Activation, LSTM, Sigmoid Activation, ConvLSTM, Convolution",LSTM,ConvLSTM,,,,
twitter-sentiment-analysis-lexicon-method,SVM,SVM,,,,,
deepsat-a-learning-framework-for-satellite,Deep Belief Network,Deep Belief Network,,,,,
asynchronous-distributed-admm-for-large-scale,"Logistic Regression, SPEED, ADMM",Logistic Regression,,,,,
distributed-machine-learning-via-sufficient,Logistic Regression,Logistic Regression,,,,,
machine-learning-methods-to-analyze,SVM,SVM,,,,,
evaluating-classifiers-in-detecting-419-scams,SVM,SVM,,,,,
artificial-prediction-markets-for-online,Q-Learning,Q-Learning,,,,,
multi-objective-optimization-for-self,"Denoising Autoencoder, AutoEncoder",Autoencoder,,,,,
local-nonstationarity-for-efficient-bayesian,Gaussian Process,Gaussian Process,,,,,
bayesian-optimization-for-materials-design,Gaussian Process,Gaussian Process,,,,,
probabilistic-network-metrics-variational,Gaussian Process,Gaussian Process,,,,,
f-svm-combination-of-feature-transformation,SVM,SVM,,,,,
analysis-of-spectrum-occupancy-using-machine,SVM,SVM,,,,,
a-randomized-nonmonotone-block-proximal,SVM,SVM,,,,,
polarization-measurement-of-high-dimensional,SVM,SVM,,,,,
latent-gaussian-processes-for-distribution,Gaussian Process,Gaussian Process,,,,,
convolutional-lstm-networks-for-subcellular,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
heteroscedastic-treed-bayesian-optimisation,Gaussian Process,Gaussian Process,,,,,
large-scale-purchase-prediction-with,Linear Regression,Linear Regression,,,,,
a-partan-accelerated-frank-wolfe-algorithm,SVM,SVM,,,,,
a-scaled-gradient-projection-method-for,Gaussian Process,Gaussian Process,,,,,
incremental-majorization-minimization,Logistic Regression,Logistic Regression,,,,,
deep-learning-with-nonparametric-clustering,Deep Belief Network,Deep Belief Network,,,,,
communication-efficient-distributed,Logistic Regression,Logistic Regression,,,,,
deep-belief-network-training-improvement,SVM,SVM,,,,,
supervised-learning-methods-for-bangla-web,SVM,SVM,,,,,
variational-inference-for-uncertainty-on-the,Gaussian Process,Gaussian Process,,,,,
dusk-a-dual-structure-preserving-kernel-for,SVM,SVM,,,,,
automated-machine-learning-on-big-data-using,Gaussian Process,Gaussian Process,,,,,
mind-the-nuisance-gaussian-process,Gaussian Process,Gaussian Process,,,,,
fast-support-vector-machines-using-parallel,SVM,SVM,,,,,
freeze-thaw-bayesian-optimization,Gaussian Process,Gaussian Process,,,,,
layered-logic-classifiers-exploring-the-and,SVM,SVM,,,,,
bayesian-inference-for-gaussian-process,Gaussian Process,Gaussian Process,,,,,
an-equivalence-between-the-lasso-and-support,SVM,SVM,,,,,
learning-optimization-models-in-the-presence,Linear Regression,Linear Regression,,,,,
a-clockwork-rnn,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
probabilistic-solutions-to-differential,Gaussian Process,Gaussian Process,,,,,
mirna-and-gene-expression-based-cancer,SVM,SVM,,,,,
manifold-regularized-kernel-logistic,"Logistic Regression, SVM",SVM,Logistic Regression,,,,
the-matrix-ridge-approximation-algorithms-and,"Gaussian Process, Spectral Clustering",Gaussian Process,,,,,
a-mapreduce-based-distributed-svm-algorithm,SVM,SVM,,,,,
automated-classification-of-lr-hand-movement,SVM,SVM,,,,,
image-forgery-detection-based-on-the-fusion,SVM,SVM,,,,,
multilabel-classification-through-random,SVM,SVM,,,,,
a-novel-frank-wolfe-algorithm-analysis-and,SVM,SVM,,,,,
flexible-high-dimensional-classification,SVM,SVM,,,,,
accelerated-proximal-stochastic-dual,SVM,SVM,,,,,
structured-learning-of-sum-of-submodular,SVM,SVM,,,,,
visual-saliency-estimation-by-integrating,SVM,SVM,,,,,
modeling-the-stable-operating-envelope-for,SVM,SVM,,,,,
fast-dual-variational-inference-for-non,Gaussian Process,Gaussian Process,,,,,
learning-from-imprecise-and-fuzzy,Logistic Regression,Logistic Regression,,,,,
fixed-rank-matrix-factorizations-and,Linear Regression,Linear Regression,,,,,
experiments-using-belief-functions-and,Logistic Regression,Logistic Regression,,,,,
practical-bayesian-optimization-of-machine,Gaussian Process,Gaussian Process,,,,,
auto-meta-automated-gradient-based-meta,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
generative-adversarial-networks-for-image-to,"Cycle Consistency Loss, GAN Least Squares Loss, ReLU, Batch Normalization, Tanh Activation, Residual Connection, CycleGAN, Sigmoid Activation, Leaky ReLU, PatchGAN, Instance Normalization, GAN, Convolution, Residual Block",GAN,,,,,
grcan-gradient-boost-convolutional,AutoEncoder,Autoencoder,,,,,
compact-deep-neural-networks-for,SVM,SVM,,,,,
bayesian-deep-learning-on-a-quantum-computer,Gaussian Process,Gaussian Process,,,,,
adversarial-robustness-toolbox-v040,Logistic Regression,Logistic Regression,,,,,
when-gaussian-process-meets-big-data-a-review,Gaussian Process,Gaussian Process,,,,,
breast-cancer-diagnosis-via-classification,"Logistic Regression, SVM",SVM,Logistic Regression,,,,
a-tutorial-on-bayesian-optimization,Gaussian Process,Gaussian Process,,,,,
optimistic-mirror-descent-in-saddle-point,"GAN, Convolution",GAN,,,,,
gaussian-processes-and-kernel-methods-a,Gaussian Process,Gaussian Process,,,,,
ensemble-kalman-filtering-for-online-gaussian,Gaussian Process,Gaussian Process,,,,,
seq2seq2sentiment-multimodal-sequence-to,"Tanh Activation, LSTM, Sigmoid Activation, Seq2Seq",Seq2Seq,LSTM,,,,
emotion-recognition-from-speech-based-on,SVM,SVM,,,,,
a-feature-agnostic-approach-for-glaucoma,Logistic Regression,Logistic Regression,,,,,
a-dataset-of-laryngeal-endoscopic-images-with,"Dilated Convolution, ENet Dilated Bottleneck, ENet, SpatialDropout, PReLU, Softmax, Convolution, Max Pooling, SegNet, ENet Initial Block, 1x1 Convolution, ReLU, ENet Bottleneck, Kaiming Initialization, Batch Normalization",SegNet,,,,,
machine-learning-classifiers-do-not-improve,Logistic Regression,Logistic Regression,,,,,
emulating-malware-authors-for-proactive,"GAN, Convolution",GAN,,,,,
automatic-identification-of-ineffective,Logistic Regression,Logistic Regression,,,,,
comparative-study-of-discrete-wavelet,Logistic Regression,Logistic Regression,,,,,
propensity-score-estimation-using,Logistic Regression,Logistic Regression,,,,,
meta-learning-autoencoders-for-few-shot,AutoEncoder,Autoencoder,,,,,
comparison-of-methods-for-early-readmission,Logistic Regression,Logistic Regression,,,,,
towards-machine-learning-on-data-from,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
machine-learning-of-space-fractional,Gaussian Process,Gaussian Process,,,,,
classification-of-building-information-model,SVM,SVM,,,,,
the-impact-of-imbalanced-training-data-on,Logistic Regression,Logistic Regression,,,,,
global-convergence-to-the-equilibrium-of-gans,"GAN, Convolution",GAN,,,,,
building-a-kannada-pos-tagger-using-machine,"Tanh Activation, CRF, LSTM, Sigmoid Activation, Seq2Seq",Seq2Seq,LSTM,,,,
fundamentals-of-recurrent-neural-network-rnn,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
code-mixed-sentiment-analysis-using-machine,"Logistic Regression, SVM",SVM,Logistic Regression,,,,
cache-telepathy-leveraging-shared-resource,"ResNet, Average Pooling, Dense Connections, Global Average Pooling, VGG, Residual Connection, Bottleneck Residual Block, Convolution, Softmax, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Dropout, Batch Normalization",VGG,ResNet,,,,
deep-ehr-chronic-disease-prediction-using,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
robust-training-of-recurrent-neural-networks,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
fast-and-accurate-convolutional-neural,"FPN, Convolution, Focal Loss, 1x1 Convolution, RetinaNet",RetinaNet,,,,,
synthetic-patient-generation-a-deep-learning,VAE,VAE,,,,,
predictslums-a-new-model-for-identifying-and,Logistic Regression,Logistic Regression,,,,,
scalable-population-synthesis-with-deep,"AutoEncoder, VAE",VAE,Autoencoder,,,,
memory-time-span-in-lstms-for-multi-speaker,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
extracting-sentiment-attitudes-from,SVM,SVM,,,,,
generalisation-in-humans-and-deep-neural,VGG-19,VGG,VGG-19,,,,
ensemble-learning-applied-to-classify-gps,Gaussian Process,Gaussian Process,,,,,
total-recall-understanding-traffic-signs,SVM,SVM,,,,,
stellar-cluster-detection-using-gmm-with-deep,AutoEncoder,Autoencoder,,,,,
generating-highly-realistic-images-of-skin,"Leaky ReLU, DCGAN, LAPGAN, GAN, Convolution, Laplacian Pyramid, ReLU, Batch Normalization",GAN,DCGAN,,,,
an-efficient-approach-for-polyps-detection-in,"RPN, RoIPool, Convolution, Softmax, Faster R-CNN",CNN,Faster R-CNN,,,,
road-user-abnormal-trajectory-detection-using,AutoEncoder,Autoencoder,,,,,
travel-speed-prediction-with-a-hierarchical,"Tanh Activation, LSTM, Sigmoid Activation, SPEED",LSTM,,,,,
discovering-influential-factors-in,VAE,VAE,,,,,
multi-view-factorization-autoencoder-with,AutoEncoder,Autoencoder,,,,,
an-analysis-of-hierarchical-text,"fastText, SVM",SVM,,,,,
estimate-the-warfarin-dose-by-ensemble-of,Linear Regression,Linear Regression,,,,,
anomaly-detection-with-generative-adversarial,"GAN, Convolution",GAN,,,,,
study-and-observation-of-the-variation-of,SVM,SVM,,,,,
inferring-political-alignments-of-twitter,SVM,SVM,,,,,
a-generalized-representer-theorem-for-hilbert,Gaussian Process,Gaussian Process,,,,,
detecting-hate-speech-and-offensive-language,SVM,SVM,,,,,
graph-laplacian-regularized-graph,GCN,GCN,,,,,
eddi-efficient-dynamic-discovery-of-high,AutoEncoder,Autoencoder,,,,,
set-transformer-a-framework-for-attention,"Dense Connections, Set Transformer, Label Smoothing, Transformer, Multi-Head Attention, Gaussian Process, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, ReLU, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,Gaussian Process,,,,
taming-vaes,"GECO, VAE",VAE,,,,,
detecting-satire-in-the-news-with-machine,Logistic Regression,Logistic Regression,,,,,
optimization-algorithm-inspired-deep-neural,"ResNet, Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, DenseNet, Residual Connection, Bottleneck Residual Block, Convolution, Residual Block, Softmax, Max Pooling, 1x1 Convolution, ReLU, Dense Block, Kaiming Initialization, Dropout, Batch Normalization",ResNet,DenseNet,,,,
band-assignment-in-dual-band-systems-a,Logistic Regression,Logistic Regression,,,,,
a-machine-learning-based-recommendation,Linear Regression,Linear Regression,,,,,
on-breimans-dilemma-in-neural-networks-phase,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout, Grouped Convolution",AlexNet,,,,,
is-your-statement-purposeless-predicting,SVM,SVM,,,,,
feature-learning-for-fault-detection-in-high,Deep Belief Network,Deep Belief Network,,,,,
mdgan-boosting-anomaly-detection-using-multi,"AutoEncoder, GAN, Convolution",GAN,Autoencoder,,,,
on-kernel-derivative-approximation-with,"PCA, SVM",SVM,,,,,
hunting-for-discriminatory-proxies-in-linear,Linear Regression,Linear Regression,,,,,
sleep-arousal-detection-from-polysomnography,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
gan-augmentation-augmenting-training-data,"GAN, Convolution",GAN,,,,,
change-surfaces-for-expressive,Gaussian Process,Gaussian Process,,,,,
distributive-dynamic-spectrum-access-through,Q-Learning,Q-Learning,,,,,
long-short-term-attention,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
unsupervised-identification-of-disease-marker,AutoEncoder,Autoencoder,,,,,
accelerating-stochastic-training-for-over,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Adam, Kaiming Initialization, Batch Normalization, Residual Connection, Bottleneck Residual Block, ResNet, SGD, Average Pooling",ResNet,,,,,
cross-lingual-transfer-learning-for,"Tanh Activation, LSTM, Sigmoid Activation, ELMo, Softmax, BiLSTM",ELMo,LSTM,BiLSTM,,,
structure-learning-of-deep-neural-networks,"GoogLeNet, Average Pooling, Dense Connections, Inception Module, Auxiliary Classifier, Residual Connection, Q-Learning, Convolution, Residual Block, Softmax, Local Response Normalization, Max Pooling, 1x1 Convolution, ReLU, Dropout, Batch Normalization",GoogleNet,Q-Learning,LeNet,,,
visual-attention-network-for-low-dose-ct,"GAN, Convolution",GAN,,,,,
improving-robustness-of-attention-models-on,GAT,GAT,,,,,
algorithms-for-screening-of-cervical-cancer-a,"k-NN, SVM",SVM,,,,,
reinforcement-learning-based-dynamic-model,Q-Learning,Q-Learning,,,,,
pilae-a-non-gradient-descent-learning-scheme,AutoEncoder,Autoencoder,,,,,
security-for-machine-learning-based-systems,"Dense Connections, Convolution, LeNet",LeNet,,,,,
large-scale-heteroscedastic-regression-via,Gaussian Process,Gaussian Process,,,,,
real-time-traffic-data-prediction-with-basic,"Tanh Activation, LSTM, Sigmoid Activation, SPEED",LSTM,,,,,
md-gan-multi-discriminator-generative,"GAN, Convolution",GAN,,,,,
identification-of-internal-faults-in-indirect,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
multi-source-neural-variational-inference,AutoEncoder,Autoencoder,,,,,
reset-learning-recurrent-dynamic-routing-in,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Residual Connection, Bottleneck Residual Block, ResNet, Average Pooling",ResNet,,,,,
gpipe-efficient-training-of-giant-neural,"Average Pooling, Dense Connections, Label Smoothing, AmoebaNet, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Convolution, Max Pooling, Spatially Separable Convolution, Position-Wise Feed-Forward Layer, Adam, ReLU, GPipe, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
synthetic-lung-nodule-3d-image-generation,AutoEncoder,Autoencoder,,,,,
model-change-detection-with-application-to,Logistic Regression,Logistic Regression,,,,,
dancing-in-the-dark-private-multi-party,Logistic Regression,Logistic Regression,,,,,
combining-deep-learning-and-qualitative,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
gansfer-learning-combining-labelled-and,"GAN, Convolution",GAN,,,,,
few-shot-generalization-across-dialogue-tasks,"Content-based Attention, Tanh Activation, LSTM, Sigmoid Activation, Softmax, Location-based Attention, Neural Turing Machine",LSTM,,,,,
a-deep-latent-variable-model-application-to,AutoEncoder,Autoencoder,,,,,
multi-scale-distributed-representation-for,"Attention Gate, Deep Ensembles, Attention Mesh",Ensemble,GAT,,,,
beyond-inferring-class-representatives-user,"GAN, Convolution",GAN,,,,,
a-system-for-automated-image-editing-from,"Tanh Activation, LSTM, Sigmoid Activation, SVM",SVM,LSTM,,,,
feature-selection-based-on-unique-relevant,"PCA, AutoEncoder",Autoencoder,,,,,
wireless-data-acquisition-for-edge-learning,SVM,SVM,,,,,
gadget-svm-a-gossip-based-sub-gradient-solver,SVM,SVM,,,,,
combatting-adversarial-attacks-through,AutoEncoder,Autoencoder,,,,,
eeg-classification-based-on-image,SVM,SVM,,,,,
generation-of-synthetic-electronic-medical,"REINFORCE, GAN, Convolution",GAN,,,,,
utilizing-imbalanced-data-and-classification,SVM,SVM,,,,,
voice-disorder-detection-using-long-short,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
the-limitations-of-model-uncertainty-in,Gaussian Process,Gaussian Process,,,,,
deep-anomaly-detection-with-outlier-exposure,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Bottleneck Residual Block, Residual Connection, ResNet, Average Pooling",ResNet,,,,,
algorithmic-theory-of-odes-and-sampling-from,Logistic Regression,Logistic Regression,,,,,
ecological-data-analysis-based-on-machine,Logistic Regression,Logistic Regression,,,,,
somm-into-the-model,"Tanh Activation, LSTM, Sigmoid Activation, BiLSTM",LSTM,BiLSTM,,,,
scalable-wide-and-deep-learning-for-computer,Logistic Regression,Logistic Regression,,,,,
plusemo2vec-at-semeval-2018-task-1-exploiting-1,Logistic Regression,Logistic Regression,,,,,
mitre-at-semeval-2017-task-1-simple-semantic,Linear Regression,Linear Regression,,,,,
svnit-semeval-2017-task-6-learning-a-sense-of,SVM,SVM,,,,,
hlpupenn-at-semeval-2017-task-4a-a-simple,SVM,SVM,,,,,
duth-at-semeval-2017-task-5-sentiment,Linear Regression,Linear Regression,,,,,
nilc-at-cwi-2018-exploring-feature,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
named-entity-recognition-for-hindi-english,"Tanh Activation, CRF, LSTM, Sigmoid Activation",LSTM,,,,,
economic-event-detection-in-company-specific,SVM,SVM,,,,,
aggressive-language-identification-using-word,CharacterBERT,BERT,,,,,
merging-datasets-for-aggressive-text,Logistic Regression,Logistic Regression,,,,,
automatic-identification-of-drugs-and-adverse,"Tanh Activation, LSTM, Sigmoid Activation, BiLSTM, SVM",SVM,LSTM,BiLSTM,,,
a-hybrid-pipeline-of-rules-and-machine,Logistic Regression,Logistic Regression,,,,,
the-use-of-object-labels-and-spatial,"R-CNN, Convolution, Max Pooling, SVM",SVM,CNN,,,,
automatic-classification-of-doctor-patient,SVM,SVM,,,,,
predicting-news-values-from-headline-text-and,SVM,SVM,,,,,
dmgroup-at-emoint-2017-emotion-intensity,Linear Regression,Linear Regression,,,,,
automatic-construction-of-large-readability,Logistic Regression,Logistic Regression,,,,,
qcri-dsl-2016-spoken-arabic-dialect,"Logistic Regression, SVM",SVM,Logistic Regression,,,,
cancer-hallmark-text-classification-using,SVM,SVM,,,,,
attending-sentences-to-detect-satirical-fake,"Tanh Activation, GRU, LSTM, Sigmoid Activation, SVM",SVM,GRU,LSTM,,,
iiit-h-at-ijcnlp-2017-task-4-customer,"Tanh Activation, fastText, LSTM, Sigmoid Activation, GloVe, SVM",SVM,LSTM,,,,
bml-a-high-performance-low-cost-gradient,"VGG-19, SPEED",VGG,VGG-19,,,,
best-response-regression,Linear Regression,Linear Regression,,,,,
bayesian-optimization-with-robust-bayesian,Gaussian Process,Gaussian Process,,,,,
dual-decomposed-learning-with-factorwise,SVM,SVM,,,,,
sparse-linear-programming-via-primal-and-dual,SVM,SVM,,,,,
learning-structured-output-representation,"Stochastic Gradient Variational Bayes, cVAE",VAE,,,,,
communication-efficient-distributed-machine,"Logistic Regression, ICA",Logistic Regression,,,,,
high-dimensional-gaussian-process-bandits,Gaussian Process,Gaussian Process,,,,,
distributed-submodular-maximization-1,Gaussian Process,Gaussian Process,,,,,
kernel-latent-svm-for-visual-recognition,SVM,SVM,,,,,
dimensionality-dependent-pac-bayes-margin,SVM,SVM,,,,,
the-lovasz-function-svms-and-finding-large,SVM,SVM,,,,,
non-asymptotic-analysis-of-stochastic,Logistic Regression,Logistic Regression,,,,,
variational-gaussian-process-dynamical,Gaussian Process,Gaussian Process,,,,,
self-paced-learning-for-latent-variable,SVM,SVM,,,,,
identifying-patients-at-risk-of-major-adverse,SVM,SVM,,,,,
ranking-measures-and-loss-functions-in,SVM,SVM,,,,,
privacy-preserving-logistic-regression,Logistic Regression,Logistic Regression,,,,,
a-massively-parallel-digital-learning,"SPEED, SVM",SVM,,,,,
modeling-human-function-learning-with,"Gaussian Process, Linear Regression",Linear Regression,Gaussian Process,,,,
coding-kendalls-shape-trajectories-for-3d,"Tanh Activation, LSTM, Sigmoid Activation, SVM",SVM,LSTM,,,,
state-frequency-memory-recurrent-neural,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
high-dimensional-bayesian-optimization-with,Gaussian Process,Gaussian Process,,,,,
structured-variationally-auto-encoded,Gaussian Process,Gaussian Process,,,,,
autoprognosis-automated-clinical-prognostic-1,Gaussian Process,Gaussian Process,,,,,
randomized-block-cubic-newton-method,Logistic Regression,Logistic Regression,,,,,
aligning-artificial-neural-networks-to-the,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout, Grouped Convolution",AlexNet,,,,,
rethinking-learning-rate-schedules-for,Linear Regression,Linear Regression,,,,,
optimistic-mirror-descent-in-saddle-point-1,"GAN, Convolution",GAN,,,,,
understand-the-dynamics-of-gans-via-primal,"GAN, Convolution",GAN,,,,,
learning-latent-semantic-representation-from,"GAN, VAE, Convolution",GAN,VAE,,,,
bigsage-unsupervised-inductive-representation,GraphSAGE,GraphSAGE,,,,,
latent-domain-transfer-crossing-modalities,AutoEncoder,Autoencoder,,,,,
an-exhaustive-analysis-of-lazy-vs-eager,"Linear Regression, PCA, SVM",SVM,Linear Regression,,,,
physiological-signal-embeddings-phase-via,AutoEncoder,Autoencoder,,,,,
deep-autoencoding-gaussian-mixture-model-for,AutoEncoder,Autoencoder,,,,,
latentpoison-adversarial-attacks-on-the-1,AutoEncoder,Autoencoder,,,,,
deep-temporal-clustering-fully-unsupervised-1,AutoEncoder,Autoencoder,,,,,
parallelized-interactive-machine-learning-on,"DQN, Dense Connections, SPEED, Q-Learning, Convolution",Q-Learning,,,,,
artificial-neural-networks-condensation-a,"Pruning, Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
fault-location-in-power-distribution-systems,GCN,GCN,,,,,
towards-resilient-machine-learning-for,"GAN, Convolution",GAN,,,,,
identification-of-character-adjectives-from,Logistic Regression,Logistic Regression,,,,,
classification-of-radiology-reports-by,Logistic Regression,Logistic Regression,,,,,
learning-a-generator-model-from-terminal-bus,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
performance-of-three-slim-variants-of-the,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
soft-autoencoder-and-its-wavelet-shrinkage,AutoEncoder,Autoencoder,,,,,
solving-large-scale-l1-regularized-svms-and,SVM,SVM,,,,,
deep-reinforcement-learning-for-imbalanced,"DQN, Dense Connections, Q-Learning, Convolution",Q-Learning,,,,,
tree-tensor-networks-for-generative-modeling,"Restricted Boltzmann Machine, VAE, PixelCNN",VAE,CNN,,,,
dppnet-approximating-determinantal-point,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
choosing-the-right-word-using-bidirectional,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
short-term-forecasting-of-italian-residential,Gaussian Process,Gaussian Process,,,,,
gaussian-processes-with-linear-operator,Gaussian Process,Gaussian Process,,,,,
detecting-overfitting-of-deep-generative,"GAN, Convolution",GAN,,,,,
spectrogram-feature-losses-for-music-source,"Dense Connections, Convolution, Softmax, VGG, Max Pooling, ReLU, Dropout",VGG,,,,,
mad-gan-multivariate-anomaly-detection-for,"GAN, Convolution",GAN,,,,,
tactilegcn-a-graph-convolutional-network-for,Graph Neural Network,GNN,,,,,
protein-classification-using-machine-learning,SVM,SVM,,,,,
exploring-semi-supervised-variational,AutoEncoder,Autoencoder,,,,,
a-semi-supervised-machine-learning-approach,Logistic Regression,Logistic Regression,,,,,
the-autofeat-python-library-for-automatic,Linear Regression,Linear Regression,,,,,
synthesizing-facial-photometries-and,"GAN, Convolution",GAN,,,,,
evaluating-the-state-of-the-art-of-end-to-end,"Tanh Activation, LSTM, Sigmoid Activation, Seq2Seq",Seq2Seq,LSTM,,,,
using-cyclegans-for-effectively-reducing,"Cycle Consistency Loss, GAN Least Squares Loss, ReLU, Batch Normalization, Tanh Activation, Residual Connection, CycleGAN, Sigmoid Activation, Leaky ReLU, PatchGAN, Instance Normalization, Convolution, Residual Block",GAN,,,,,
extracting-pico-elements-from-rct-abstracts,SVM,SVM,,,,,
deep-learning-on-attributed-graphs-a-journey,AutoEncoder,Autoencoder,,,,,
deep-mean-functions-for-meta-learning-in,"Gaussian Process, PCA",Gaussian Process,,,,,
support-feature-machines,SVM,SVM,,,,,
deep-neural-decision-forests,"GoogLeNet, Average Pooling, Dense Connections, Inception Module, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout, Auxiliary Classifier",GoogleNet,LeNet,,,,
steerable-wavelet-scattering-for-3d-atomic,Linear Regression,Linear Regression,,,,,
heartbeat-anomaly-detection-using-adversarial,"Feedforward Network, InfoGAN, SMOTE, Leaky ReLU, Dense Connections, Softmax, ReLU",GAN,,,,,
short-term-demand-forecasting-for-online-car,"Tanh Activation, GRU, LSTM, Sigmoid Activation",GRU,LSTM,,,,
deep-recurrent-factor-model-interpretable-non,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
compressing-gans-using-knowledge-distillation,"GAN, Convolution",GAN,,,,,
hop-heterogeneity-aware-decentralized,SVM,SVM,,,,,
right-for-the-wrong-reasons-diagnosing,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
universal-lemmatizer-a-sequence-to-sequence,AutoEncoder,Autoencoder,,,,,
medical-diagnosis-with-a-novel-svm-codoa,SVM,SVM,,,,,
temporal-convolutional-networks-and-dynamic,Gaussian Process,Gaussian Process,,,,,
polyphonic-music-composition-with-lstm-neural,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
qp-wasserstein-gans-comparing-ground-metrics,"WGAN, Convolution",GAN,,,,,
assessing-the-local-interpretability-of,Logistic Regression,Logistic Regression,,,,,
yes-we-gan-applying-adversarial-techniques,"GAN, Convolution",GAN,,,,,
extended-2d-volumetric-consensus-hippocampus,"Dense Connections, U-Net, Concatenated Skip Connection, Convolution, Softmax, VGG, Max Pooling, ReLU, Dropout, Batch Normalization",VGG,,,,,
scaling-limits-of-wide-neural-networks-with,"Gaussian Process, SGD",Gaussian Process,,,,,
differential-description-length-for,"Linear Regression, MDL",Linear Regression,,,,,
progressively-growing-generative-adversarial,"GAN, Convolution",GAN,,,,,
improving-deep-image-clustering-with-spatial,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Spatial Transformer, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
svm-based-deep-stacking-networks,SVM,SVM,,,,,
prediction-of-porosity-and-permeability,Linear Regression,Linear Regression,,,,,
learning-compositional-representations-of,"PCA, Restricted Boltzmann Machine, VAE, ICA",VAE,,,,,
a-semi-supervised-deep-residual-network-for,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Residual Connection, Bottleneck Residual Block, ResNet, Average Pooling",ResNet,,,,,
multi-agent-dual-learning,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
revealing-quantum-chaos-with-machine-learning,AutoEncoder,Autoencoder,,,,,
nonlinear-generalization-of-the-single-index,Linear Regression,Linear Regression,,,,,
when-relaxations-go-bad-differentially,Logistic Regression,Logistic Regression,,,,,
a-new-approach-to-automatic-disc-localization,SVM,SVM,,,,,
virtual-adversarial-training-on-graph,GCN,GCN,,,,,
enhancing-the-robustness-of-deep-neural,"GAN, Convolution",GAN,,,,,
deep-learning-and-gaussian-process-based-band,Gaussian Process,Gaussian Process,,,,,
financial-series-prediction-using-attention,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
stability-of-decision-trees-and-logistic,Logistic Regression,Logistic Regression,,,,,
gap-generalizable-approximate-graph,"ResNet, Average Pooling, Dense Connections, Global Average Pooling, VGG, Residual Connection, Bottleneck Residual Block, Residual Block, Convolution, Softmax, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Dropout, Batch Normalization",VGG,ResNet,,,,
high-fidelity-image-generation-with-fewer,"BigGAN, GAN Hinge Loss, Spectral Normalization, Dense Connections, Projection Discriminator, Non-Local Block, Residual Connection, Feedforward Network, TTUR, Conditional Batch Normalization, SAGAN Self-Attention Module, Early Stopping, SAGAN, Dot-Product Attention, Convolution, Residual Block, Softmax, Non-Local Operation, 1x1 Convolution, Adam, Off-Diagonal Orthogonal Regularization, ReLU, Batch Normalization, Linear Layer, Truncation Trick",GAN,BigGAN,,,,
representative-task-self-selection-for,AutoEncoder,Autoencoder,,,,,
learning-heuristics-over-large-graphs-via,Q-Learning,Q-Learning,,,,,
nowcasting-recessions-using-the-svm-machine,SVM,SVM,,,,,
program-classification-using-gated-graph,Graph Neural Network,GNN,,,,,
machine-learning-based-prediction-and,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
financial-applications-of-gaussian-processes,Gaussian Process,Gaussian Process,,,,,
personal-dynamic-cost-aware-sensing-for,"Linear Regression, AutoEncoder",Linear Regression,Autoencoder,,,,
gee-a-gradient-based-explainable-variational,AutoEncoder,Autoencoder,,,,,
dyslexml-screening-tool-for-dyslexia-using,SVM,SVM,,,,,
inefficiency-of-k-fac-for-large-batch-size,"ResNet, Average Pooling, Dense Connections, AlexNet, Global Average Pooling, Grouped Convolution, Residual Connection, Bottleneck Residual Block, SGD, Convolution, Residual Block, Softmax, Local Response Normalization, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Dropout, Batch Normalization",ResNet,AlexNet,,,,
deep-gaussian-processes-for-multi-fidelity,Gaussian Process,Gaussian Process,,,,,
emotion-recognition-with-machine-learning,SVM,SVM,,,,,
training-recurrent-neural-networks-robust-to,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
automatic-classification-of-pathology-reports,"Logistic Regression, SVM",SVM,Logistic Regression,,,,
improved-robustness-of-reinforcement-learning,"Q-Learning, ReLU",Q-Learning,,,,,
signal-demodulation-with-machine-learning,Deep Belief Network,Deep Belief Network,,,,,
sentiment-analysis-on-imdb-movie-comments-and,SVM,SVM,,,,,
interpretation-of-machine-learning,Logistic Regression,Logistic Regression,,,,,
detecting-activities-of-daily-living-and,SVM,SVM,,,,,
a-proof-of-convergence-of-multi-class,Logistic Regression,Logistic Regression,,,,,
question-embeddings-based-on-shannon-entropy,"Logistic Regression, fastText",Logistic Regression,,,,,
data-driven-approximation-of-parametrized,AutoEncoder,Autoencoder,,,,,
empirical-study-of-deep-learning-for-text,"Logistic Regression, SVM",SVM,Logistic Regression,,,,
identifying-disease-free-chest-x-ray-images,"Residual Connection, Average Pooling, Inception-ResNet-v2-C, Reduction-A, Convolution, Softmax, Inception-ResNet-v2 Reduction-B, Inception-ResNet-v2-A, Max Pooling, Inception-ResNet-v2-B, Inception-ResNet-v2, 1x1 Convolution, ReLU, Dropout",ResNet,,,,,
bolasso-model-consistent-lasso-estimation,Linear Regression,Linear Regression,,,,,
neural-networks-for-modeling-source-code,"Pointer Network, Tanh Activation, LSTM, Sigmoid Activation, Additive Attention, Softmax",LSTM,,,,,
a-review-on-neural-turing-machine,"Content-based Attention, Tanh Activation, LSTM, Sigmoid Activation, Softmax, Location-based Attention, Neural Turing Machine",LSTM,,,,,
ranking-based-autoencoder-for-extreme-multi,AutoEncoder,Autoencoder,,,,,
measuring-the-influence-of-mere-exposure,SVM,SVM,,,,,
at-gan-a-generative-attack-model-for,"GAN, Convolution",GAN,,,,,
asd-diagnet-a-hybrid-learning-approach-for,AutoEncoder,Autoencoder,,,,,
examining-the-capability-of-gans-to-replace,"Leaky ReLU, DCGAN, Convolution, ReLU, Batch Normalization",GAN,DCGAN,,,,
an-empirical-evaluation-of-text,"Weight Tying, Adam, ULMFiT, Dropout, Variational Dropout, Discriminative Fine-Tuning, DropConnect, LSTM, Slanted Triangular Learning Rates, Temporal Activation Regularization, AWD-LSTM, Tanh Activation, Embedding Dropout, fastText, Sigmoid Activation, Activation Regularization, Skip-gram Word2Vec, GloVe",LSTM,,,,,
190408484,"RPN, RoIPool, Softmax, Convolution, Faster R-CNN",CNN,Faster R-CNN,,,,
data-augmentation-using-gans,"SMOTE, GAN, Convolution",GAN,,,,,
advanced-deep-convolutional-neural-network,"DCNN, U-Net, Concatenated Skip Connection, Convolution, Max Pooling, ReLU",CNN,DCNN,,,,
towards-learning-of-filter-level,"Pruning, Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
distributed-generation-of-privacy-preserving,"AutoEncoder, VAE",VAE,Autoencoder,,,,
identifying-cross-country-skiing-techniques,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
estimating-forces-of-robotic-pouring-using-a,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
horseshoe-regularization-for-machine-learning,Linear Regression,Linear Regression,,,,,
time-series-simulation-by-conditional,"GAN, Convolution",GAN,,,,,
treegrad-transferring-tree-ensembles-to,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
d-vae-a-variational-autoencoder-for-directed,"Tanh Activation, LSTM, Sigmoid Activation, AutoEncoder, Softmax",LSTM,Autoencoder,,,,
comparing-machine-learning-models-to-choose,SVM,SVM,,,,,
a-robust-approach-for-securing-audio,SVM,SVM,,,,,
robustness-verification-of-support-vector,SVM,SVM,,,,,
robust-metric-learning-based-on-the-rescaled,SVM,SVM,,,,,
sealion-a-framework-for-neural-network,AutoEncoder,Autoencoder,,,,,
hog-feature-extraction-from-encrypted-images,SVM,SVM,,,,,
softmax-optimizations-for-intel-xeon,"Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer, Attention Dropout, GELU, Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, WordPiece, Residual Connection",BERT,Transformer,,,,
190412613,"Dense Connections, Convolution, Softmax, VGG, Max Pooling, ReLU, Dropout",VGG,,,,,
190412617,SVM,SVM,,,,,
sequence-to-sequence-deep-learning-models-for,"Tanh Activation, LSTM, Sigmoid Activation, SPEED",LSTM,,,,,
high-performance-support-vector-machines-and,SVM,SVM,,,,,
machine-learning-for-classification-of,"Tanh Activation, LSTM, Sigmoid Activation, BiLSTM",LSTM,BiLSTM,,,,
learning-some-popular-gaussian-graphical,Linear Regression,Linear Regression,,,,,
distance-metric-learned-collaborative,VGG-19,VGG,VGG-19,,,,
charbot-a-simple-and-effective-method-for,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
network-representation-learning-consolidation,"WYS, DeepWalk, GraRep, VERSE, SDNE, node2vec, LapEigen, MNMF, HOPE, NetMF, VGAE",SDNE,,,,,
meta-learners-learning-dynamics-are-unlike,"Linear Regression, Tanh Activation, LSTM, Sigmoid Activation",Linear Regression,LSTM,,,,
generalization-ability-of-region-proposal,"DCNN, RPN, RoIPool, Convolution, Softmax, Faster R-CNN",Faster R-CNN,CNN,DCNN,,,
190503406,Gaussian Process,Gaussian Process,,,,,
budgeted-training-rethinking-deep-neural,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
on-graph-classification-networks-datasets-and,GCN,GCN,,,,,
using-syntactical-and-logical-forms-to,"Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer, Attention Dropout, GELU, Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, WordPiece, Residual Connection",BERT,Transformer,,,,
190505700,"Tanh Activation, GRU, LSTM, BiGRU, Sigmoid Activation, BiLSTM",BiGRU,GRU,LSTM,BiLSTM,,
online-anomaly-detection-with-sparse-gaussian,Gaussian Process,Gaussian Process,,,,,
a-deep-learning-approach-to-detecting-volcano,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout, Grouped Convolution",AlexNet,,,,,
predicting-solar-flares-using-a-long-short,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
exploring-helical-dynamos-with-machine,Linear Regression,Linear Regression,,,,,
spatio-temporal-adversarial-learning-for,"AutoEncoder, Convolution",Autoencoder,,,,,
disentangled-attribution-curves-for,"Logistic Regression, Disentangled Attribution Curves",Logistic Regression,,,,,
ae2-nets-autoencoder-in-autoencoder-networks,AutoEncoder,Autoencoder,,,,,
attack-and-anomaly-detection-in-iot-sensors,Logistic Regression,Logistic Regression,,,,,
exploring-bias-in-gan-based-data-augmentation,"GAN, Convolution",GAN,,,,,
a-text-classification-framework-for-simple,SVM,SVM,,,,,
revisiting-graph-neural-networks-all-we-have,Graph Neural Network,GNN,,,,,
robust-variational-autoencoder,"AutoEncoder, VAE",VAE,Autoencoder,,,,
practical-and-consistent-estimation-of-f,AutoEncoder,Autoencoder,,,,,
communication-efficient-distributed-blockwise,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Residual Connection, Bottleneck Residual Block, ResNet, SGD, Average Pooling",ResNet,,,,,
semi-supervised-gan-for-classification-of,"GAN, Convolution",GAN,,,,,
compactnet-platform-aware-automatic,"Average Pooling, Pointwise Convolution, Convolution, Depthwise Separable Convolution, MobileNetV2, 1x1 Convolution, Depthwise Convolution, Inverted Residual Block, Batch Normalization",LeNet,MobileNetV2,,,,
solving-np-hard-problems-on-graphs-by,"Graph Neural Network, Q-Learning",GNN,Q-Learning,,,,
coset-a-benchmark-for-evaluating-neural,Graph Neural Network,GNN,,,,,
multi-modal-graph-interaction-for-multi-graph,"GCN, Convolution",GCN,,,,,
sparse-sparse-architecture-search-for-cnns-on,"Pruning, Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
deep-ensemble-learning-for-alzheimers-disease,"AutoEncoder, Sparse Autoencoder",Autoencoder,,,,,
the-theory-behind-overfitting-cross,"Early Stopping, SVM",SVM,,,,,
applying-generative-adversarial-networks-to,"GAN, Convolution",GAN,,,,,
menieres-disease-prognosis-by-learning-from,SVM,SVM,,,,,
190600537,"Graph Neural Network, Stochastic Depth",GNN,,,,,
190600939,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
a-study-of-feature-extraction-techniques-for,Logistic Regression,Logistic Regression,,,,,
rthn-a-rnn-transformer-hierarchical-network,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
transfer-learning-with-intelligent-training,"Dense Connections, Convolution, Softmax, VGG, Max Pooling, ReLU, Dropout",VGG,,,,,
corn-leaf-detection-using-region-based,"Global Average Pooling, Residual Block, Convolution, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Residual Connection, Bottleneck Residual Block, ResNet, Average Pooling",ResNet,,,,,
farm-land-weed-detection-with-region-based,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Dropout, Kaiming Initialization, Batch Normalization, Residual Connection, Bottleneck Residual Block, ResNet, Average Pooling",ResNet,,,,,
enumeration-of-distinct-support-vectors-for,SVM,SVM,,,,,
data-sketching-for-faster-training-of-machine,Logistic Regression,Logistic Regression,,,,,
the-unreasonable-effectiveness-of-transformer,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
improving-robustness-without-sacrificing,"Tanh Activation, LSTM, Sigmoid Activation, AutoAugment, Cutout",LSTM,,,,,
learning-to-regularize-with-a-variational,"AutoEncoder, VAE",VAE,Autoencoder,,,,
a-general-mathcalon2-hyper-parameter,Gaussian Process,Gaussian Process,,,,,
convai-at-semeval-2019-task-6-offensive,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
fermi-at-semeval-2019-task-6-identifying-and,"Tanh Activation, LSTM, Sigmoid Activation, ELMo, Softmax, BiLSTM, SVM",SVM,ELMo,LSTM,BiLSTM,,
ju_etce_17_21-at-semeval-2019-task-6,"Tanh Activation, LSTM, Sigmoid Activation, GloVe",LSTM,,,,,
ssn_nlp-at-semeval-2019-task-6-offensive,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
dbms-ku-at-semeval-2019-task-9-exploring,Logistic Regression,Logistic Regression,,,,,
naive-bayes-and-bilstm-ensemble-for,"Tanh Activation, LSTM, Sigmoid Activation, BiLSTM",LSTM,BiLSTM,,,,
bam-a-combination-of-deep-and-shallow-models,"Tanh Activation, LSTM, Sigmoid Activation, Max Pooling",LSTM,,,,,
towards-the-data-driven-system-for-rhetorical,SVM,SVM,,,,,
clpsych2019-shared-task-predicting-suicide,SVM,SVM,,,,,
libn3la-lightweight-package-for-neural-nlp,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
federated-ai-lets-a-team-imagine-together,"GAN, Convolution",GAN,,,,,
associative-convolutional-layers,"ResNet, Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, DenseNet, Residual Connection, Bottleneck Residual Block, Convolution, Residual Block, Softmax, Max Pooling, 1x1 Convolution, ReLU, Dense Block, Kaiming Initialization, Dropout, Batch Normalization",ResNet,DenseNet,,,,
leveraging-bert-for-extractive-text,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
learning-curves-for-deep-neural-networks-a,Gaussian Process,Gaussian Process,,,,,
support-vector-machine-based-fire-outbreak,SVM,SVM,,,,,
automatically-evaluating-balance-a-machine,SVM,SVM,,,,,
qbso-fs-a-reinforcement-learning-based-bee,"Q-Learning, Feature Selection",Q-Learning,,,,,
support-vector-machines-on-the-d-wave-quantum,SVM,SVM,,,,,
model-agnostic-dual-quality-assessment-for,"ResNet, Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, DenseNet, LeNet, Residual Connection, Bottleneck Residual Block, Softmax, Residual Block, Convolution, Max Pooling, 1x1 Convolution, Dense Block, ReLU, Dropout, Kaiming Initialization, Batch Normalization",LeNet,ResNet,DenseNet,,,
active-generative-adversarial-network-for,"GAN, Convolution",GAN,,,,,
petct-radiomic-sequencer-for-prediction-of,Logistic Regression,Logistic Regression,,,,,
reconciling-utility-and-membership-privacy,"Knowledge Distillation, Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, DenseNet, Softmax, Convolution, Max Pooling, 1x1 Convolution, Dense Block, ReLU, Dropout, Kaiming Initialization, Batch Normalization",DenseNet,,,,,
using-colorization-as-a-tool-for-automatic,"Colorization, GAN, Convolution",GAN,,,,,
bayesian-optimization-with-binary-auxiliary,Gaussian Process,Gaussian Process,,,,,
lia-latently-invertible-autoencoder-with,"AutoEncoder, GAN, Convolution, VAE",GAN,VAE,Autoencoder,,,
amla-an-automl-framework-for-neural-network,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
hyperopt-sklearn-automatic-hyperparameter,"PCA, SVM",SVM,,,,,
scalable-hyperparameter-optimization-with,Gaussian Process,Gaussian Process,,,,,
alchemy-a-quantum-chemistry-dataset-for,Graph Neural Network,GNN,,,,,
learning-set-equivariant-functions-with-swarm,"Dense Connections, Set Transformer, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
samplefix-learning-to-correct-programs-by,AutoEncoder,Autoencoder,,,,,
molecular-property-prediction-a-multilevel,"Graph Neural Network, SPEED",GNN,,,,,
comparing-semi-parametric-model-learning,Gaussian Process,Gaussian Process,,,,,
learning-to-identify-patients-at-risk-of,Logistic Regression,Logistic Regression,,,,,
system-misuse-detection-via-informed-behavior,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
cascade-attention-guided-residue-learning-gan,"GAN, Convolution",GAN,,,,,
globally-convergent-newton-methods-for-ill,"Logistic Regression, Softmax",Logistic Regression,,,,,
towards-interpretable-deep-extreme-multi,AutoEncoder,Autoencoder,,,,,
protecting-privacy-of-users-in-brain-computer,Linear Regression,Linear Regression,,,,,
learning-multi-party-turn-taking-models-from,SVM,SVM,,,,,
on-a-randomized-multi-block-admm-for-solving,"Linear Regression, SPEED, ADMM, SVM",SVM,Linear Regression,,,,
a-bi-directional-transformer-for-musical,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
heartbeat-classification-fusing-temporal-and,SVM,SVM,,,,,
gaussian-processes-for-analyzing-positioned,Gaussian Process,Gaussian Process,,,,,
deepacid-classification-of-macromolecule-type,"Tanh Activation, GRU, LSTM, Sigmoid Activation, Convolution",GRU,LSTM,,,,
a-stochastic-first-order-method-for-ordered,"SGD, SVM",SVM,,,,,
copula-representations-and-error-surface,"Linear Regression, ReLU",Linear Regression,,,,,
warfarin-dose-estimation-on-multiple-datasets,"Linear Regression, SVM",SVM,Linear Regression,,,,
beyond-imitation-generative-and-variational,AutoEncoder,Autoencoder,,,,,
adaptive-pricing-in-insurance-generalized,Gaussian Process,Gaussian Process,,,,,
activity2vec-learning-adl-embeddings-from,"Tanh Activation, LSTM, Sigmoid Activation, Seq2Seq",Seq2Seq,LSTM,,,,
detecting-and-simulating-artifacts-in-gan,"Cycle Consistency Loss, GAN Least Squares Loss, ReLU, Batch Normalization, Tanh Activation, Residual Connection, CycleGAN, Sigmoid Activation, Leaky ReLU, PatchGAN, Instance Normalization, GAN, Convolution, Residual Block",GAN,,,,,
a-neural-turingmachine-for-conditional,"Content-based Attention, Tanh Activation, LSTM, Sigmoid Activation, Softmax, Location-based Attention, Neural Turing Machine",LSTM,,,,,
measuring-the-transferability-of-adversarial,"ResNet, Average Pooling, Dense Connections, Global Average Pooling, VGG, Depthwise Separable Convolution, Depthwise Convolution, Xception, Residual Connection, Bottleneck Residual Block, Pointwise Convolution, Convolution, Softmax, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Dropout, Batch Normalization",Xception,VGG,ResNet,,,
counterfactual-reasoning-for-fair-clinical,AutoEncoder,Autoencoder,,,,,
the-use-of-gaussian-processes-in-system,Gaussian Process,Gaussian Process,,,,,
learning-complex-basis-functions-for,AutoEncoder,Autoencoder,,,,,
natural-adversarial-examples,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Bottleneck Residual Block, Residual Connection, ResNet, Average Pooling",ResNet,,,,,
on-the-steerability-of-generative-adversarial,"GAN, Convolution",GAN,,,,,
latent-adversarial-defence-with-boundary,SVM,SVM,,,,,
the-continuous-bernoulli-fixing-a-pervasive,VAE,VAE,,,,,
metamorphic-testing-of-a-deep-learning-based,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
low-shot-classification-a-comparison-of,"Multi-Head Attention, Temporal Activation Regularization, WordPiece, AWD-LSTM, Residual Connection, Tanh Activation, Embedding Dropout, Layer Normalization, Sigmoid Activation, Linear Warmup With Linear Decay, BERT, Activation Regularization, Scaled Dot-Product Attention, Softmax, Weight Decay, Weight Tying, Adam, ULMFiT, Dropout, Linear Layer, Variational Dropout, Discriminative Fine-Tuning, Attention Dropout, DropConnect, GELU, LSTM, Dense Connections, Slanted Triangular Learning Rates",BERT,LSTM,,,,
conversational-help-for-task-completion-and,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
design-and-evaluation-of-product-aesthetics-a,AutoEncoder,Autoencoder,,,,,
benchmarking-a-catchment-aware-long-short,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
compositional-deep-learning,"Cycle Consistency Loss, GAN Least Squares Loss, ReLU, Batch Normalization, Tanh Activation, Residual Connection, CycleGAN, Sigmoid Activation, Leaky ReLU, PatchGAN, Instance Normalization, Convolution, Residual Block",GAN,,,,,
emotion-detection-in-text-focusing-on-latent,GRU,GRU,,,,,
latent-function-decomposition-for-forecasting,Gaussian Process,Gaussian Process,,,,,
mid-price-prediction-based-on-machine,"Logistic Regression, Feature Selection",Logistic Regression,,,,,
a-hierarchically-labeled-portuguese-hate,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
nlp-automation-to-read-radiological-reports,Logistic Regression,Logistic Regression,,,,,
arabic-tweet-act-speech-act-recognition-for,SVM,SVM,,,,,
graph-informer-networks-for-molecules,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Convolution, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
is-bert-really-robust-natural-language-attack,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
q-mind-defeating-stealthy-dos-attacks-in-sdn,Q-Learning,Q-Learning,,,,,
detecting-radical-text-over-online-media,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
multi-kernel-capsule-network-for,"Capsule Network, Dropout",Capsule Network,,,,,
airbnb-price-prediction-using-machine,"Linear Regression, k-Means Clustering",Linear Regression,,,,,
featuring-the-topology-with-the-unsupervised,AutoEncoder,Autoencoder,,,,,
mixed-integer-optimization-approach-to,Logistic Regression,Logistic Regression,,,,,
machine-learning-as-ecology,SVM,SVM,,,,,
automl-a-survey-of-the-state-of-the-art,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
quantum-enhanced-least-square-support-vector,SVM,SVM,,,,,
dialogue-act-classification-in-group-chats,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
abnormality-detection-in-musculoskeletal,"Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, DenseNet, Softmax, Convolution, Max Pooling, 1x1 Convolution, Dense Block, ReLU, Dropout, Kaiming Initialization, Batch Normalization",DenseNet,,,,,
addressing-data-bias-problems-for-chest-x-ray,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
dynamic-region-division-for-adaptive-learning,"YOLOv3, Batch Normalization, Residual Connection, Logistic Regression, Average Pooling, k-Means Clustering, Global Average Pooling, Convolution, Softmax, Darknet-53, 1x1 Convolution",YOLOv3,Logistic Regression,,,,
icebreaker-element-wise-active-information,VAE,VAE,,,,,
cosmological-n-body-simulations-a-challenge,"GAN, Convolution",GAN,,,,,
predicting-eating-events-in-free-living,Logistic Regression,Logistic Regression,,,,,
performing-deep-recurrent-double-q-learning,"Tanh Activation, LSTM, Sigmoid Activation, Q-Learning, Double Q-learning, AlphaZero",Q-Learning,LSTM,,,,
ai-predicts-independent-construction-safety,SVM,SVM,,,,,
understanding-cyber-athletes-behaviour,Logistic Regression,Logistic Regression,,,,,
esports-pro-players-behavior-during-the-game,"Logistic Regression, L1 Regularization",Logistic Regression,,,,,
structural-health-monitoring-of-cantilever,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
towards-linearization-machine-learning,Logistic Regression,Logistic Regression,,,,,
contour-detection-in-cassini-iss-images-based,SVM,SVM,,,,,
adaptive-configuration-oracle-for-online,Gaussian Process,Gaussian Process,,,,,
viability-of-machine-learning-to-reduce,SVM,SVM,,,,,
generator-evaluator-selector-net-a-modular,"Pointer Network, Tanh Activation, LSTM, Sigmoid Activation, Additive Attention, Softmax",LSTM,,,,,
finbert-financial-sentiment-analysis-with-pre,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
on-the-bounds-of-function-approximations,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
improving-automatic-jazz-melody-generation-by,AutoEncoder,Autoencoder,,,,,
white-box-vs-black-box-bayes-optimal,Logistic Regression,Logistic Regression,,,,,
a-concert-planning-tool-for-independent,"Linear Regression, SGD",Linear Regression,,,,,
graph-convolutional-networks-for-road,GCN,GCN,,,,,
credit-card-fraud-detection-using-autoencoder,"Denoising Autoencoder, AutoEncoder",Autoencoder,,,,,
neural-architecture-search-for-joint,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
gadmm-fast-and-communication-efficient,Logistic Regression,Logistic Regression,,,,,
best-practices-for-scientific-research-on,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
regression-clustering-for-improved-accuracy,"Gaussian Process, Linear Regression",Linear Regression,Gaussian Process,,,,
encoders-and-decoders-for-quantum-expander,Q-Learning,Q-Learning,,,,,
robust-logistic-regression-against-attribute,Logistic Regression,Logistic Regression,,,,,
distributed-creation-of-machine-learning,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
machine-learning-accelerates-parameter,Gaussian Process,Gaussian Process,,,,,
super-learning-for-daily-streamflow,Linear Regression,Linear Regression,,,,,
arducode-predictive-framework-for-automation,AutoEncoder,Autoencoder,,,,,
machine-learning-for-stochastic,"GAN, Convolution",GAN,,,,,
evaluating-shallow-and-deep-learning,Logistic Regression,Logistic Regression,,,,,
brain-like-object-recognition-with-high,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout, Grouped Convolution",AlexNet,,,,,
transferable-adversarial-robustness-using,AutoEncoder,Autoencoder,,,,,
i-mad-a-novel-interpretable-malware-detector,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
prediction-of-rare-feature-combinations-in,"WGAN, VAE, Convolution",GAN,VAE,,,,
intelligent-active-queue-management-using,"LSTM, Q-Learning",Q-Learning,LSTM,,,,
adversarial-vulnerability-bounds-for-gaussian,Gaussian Process,Gaussian Process,,,,,
190909816,Logistic Regression,Logistic Regression,,,,,
190909852,"SPEED, SVM",SVM,,,,,
190910086,Graph Neural Network,GNN,,,,,
machine-learning-optimization-algorithms,Logistic Regression,Logistic Regression,,,,,
exploring-graph-neural-networks-for-stock,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
supervised-vector-quantized-variational,AutoEncoder,Autoencoder,,,,,
overlapping-community-detection-with-graph-1,Graph Neural Network,GNN,,,,,
unsupervised-image-translation-using,"GAN, Convolution",GAN,,,,,
classification-of-histopathological-biopsy,"Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, DenseNet, Softmax, Convolution, Max Pooling, 1x1 Convolution, Dense Block, ReLU, Dropout, Kaiming Initialization, Batch Normalization",DenseNet,,,,,
the-ant-swarm-neuro-evolution-procedure-for,"Tanh Activation, GRU, LSTM, Sigmoid Activation",GRU,LSTM,,,,
churn-prediction-with-sequential-data-and,"Tanh Activation, Logistic Regression, LSTM, Sigmoid Activation",Logistic Regression,LSTM,,,,
hatemonitors-language-agnostic-abuse,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
mgp-atttcn-an-interpretable-machine-learning,Gaussian Process,Gaussian Process,,,,,
appspred-predicting-context-aware-smartphone,Logistic Regression,Logistic Regression,,,,,
blending-diverse-physical-priors-with-neural,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
labelsens-enabling-real-time-sensor-data,"Tanh Activation, GRU, LSTM, Sigmoid Activation",GRU,LSTM,,,,
false-data-injection-attacks-in-internet-of,"Tanh Activation, GRU, LSTM, Sigmoid Activation",GRU,LSTM,,,,
generative-adversarial-networks-for-failure,"Feedforward Network, InfoGAN, Leaky ReLU, Dense Connections, GAN, Convolution, Softmax, ReLU",GAN,,,,,
pinfer-privacy-preserving-inference-for,"Logistic Regression, SVM",SVM,Logistic Regression,,,,
named-entity-recognition-is-there-a-glass,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, LSTM, Dense Connections, ELMo, BiLSTM, Multi-Head Attention, WordPiece, Residual Connection, Tanh Activation, Layer Normalization, Sigmoid Activation, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,ELMo,LSTM,BiLSTM,,
on-tractable-computation-of-expected,Logistic Regression,Logistic Regression,,,,,
torchbeast-a-pytorch-platform-for-distributed,"IMPALA, V-trace, Convolution, Max Pooling, RMSProp, ReLU, TorchBeast, Entropy Regularization, Experience Replay, Residual Connection, Tanh Activation, Gradient Clipping, LSTM, Sigmoid Activation",LSTM,,,,,
instagram-fake-and-automated-account,Logistic Regression,Logistic Regression,,,,,
adversarial-learning-of-deepfakes-in,AutoEncoder,Autoencoder,,,,,
transformers-state-of-the-art-natural,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
multi-label-categorization-of-accounts-of,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
evolving-gaussian-process-kernels-from,Gaussian Process,Gaussian Process,,,,,
generative-neural-network-based-spectrum,AutoEncoder,Autoencoder,,,,,
man-in-the-middle-attacks-against-machine,VAE,VAE,,,,,
on-solving-minimax-optimization-locally-a-1,"GAN, Convolution",GAN,,,,,
evolution-of-transfer-learning-in-natural,"Attention Dropout, DropConnect, GELU, Dense Connections, Cosine Annealing, Label Smoothing, WordPiece, Residual Connection, Embedding Dropout, Layer Normalization, Sigmoid Activation, Scaled Dot-Product Attention, Softmax, Weight Decay, Position-Wise Feed-Forward Layer, Weight Tying, Linear Warmup With Cosine Annealing, Dropout, Linear Layer, Variational Dropout, Discriminative Fine-Tuning, LSTM, ELMo, Slanted Triangular Learning Rates, Transformer, BiLSTM, GPT, Multi-Head Attention, Temporal Activation Regularization, AWD-LSTM, Tanh Activation, Linear Warmup With Linear Decay, Activation Regularization, BERT, Adam, ReLU, Absolute Position Encodings, ULMFiT, BPE",LSTM,ELMo,GPT,BERT,Transformer,BiLSTM
towards-a-precipitation-bias-corrector,"Denoising Autoencoder, AutoEncoder",Autoencoder,,,,,
knowledge-aware-autoencoders-for-explainable,AutoEncoder,Autoencoder,,,,,
many-faces-of-feature-importance-comparing,"SVM, LIME",SVM,,,,,
implicit-context-aware-learning-and-discovery,AutoEncoder,Autoencoder,,,,,
generalised-learning-of-time-series-ornstein,Gaussian Process,Gaussian Process,,,,,
bayesian-optimization-allowing-for-common,Gaussian Process,Gaussian Process,,,,,
spatiotemporal-emotion-recognition-using-deep,SVM,SVM,,,,,
resource-allocation-in-mobility-aware,"DQN, Dense Connections, Q-Learning, Convolution",Q-Learning,,,,,
q-gadmm-quantized-group-admm-for,"Linear Regression, SPEED, ADMM",Linear Regression,,,,,
train-where-the-data-is-a-case-for-bandwidth,"Logistic Regression, SVM",SVM,Logistic Regression,,,,
combining-acoustics-content-and-interaction,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
a-bert-based-transfer-learning-approach-for,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
leanconvnets-low-cost-yet-effective,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Residual Connection, Bottleneck Residual Block, ResNet, Average Pooling",ResNet,,,,,
fragment-graphical-variational-autoencoding,AutoEncoder,Autoencoder,,,,,
191013327,Linear Regression,Linear Regression,,,,,
191013315,AutoEncoder,Autoencoder,,,,,
region-based-convolution-neural-network,"Mask R-CNN, RPN, U-Net, Concatenated Skip Connection, RoIAlign, Convolution, Softmax, Max Pooling, ReLU",CNN,,,,,
hybrid-machine-learning-model-of-extreme,SVM,SVM,,,,,
safe-exploration-for-interactive-machine,Gaussian Process,Gaussian Process,,,,,
a-machine-learning-nowcasting-method-based-on,SVM,SVM,,,,,
named-entity-recognition-is-there-a-glass-1,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, LSTM, Dense Connections, ELMo, BiLSTM, Multi-Head Attention, WordPiece, Residual Connection, Tanh Activation, Layer Normalization, Sigmoid Activation, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,ELMo,LSTM,BiLSTM,,
efficient-feature-selection-techniques-for,"Logistic Regression, Feature Selection",Logistic Regression,,,,,
quantum-wasserstein-generative-adversarial,"WGAN, GAN, Convolution",GAN,,,,,
on-neural-architecture-search-for-resource,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
laplacian-smoothing-stochastic-gradient,Logistic Regression,Logistic Regression,,,,,
decoding-of-visual-related-information-from,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
detect-toxic-content-to-improve-online,"Tanh Activation, GRU, LSTM, Sigmoid Activation, GloVe, SVM",SVM,GRU,LSTM,,,
study-of-constrained-network-structures-for,"WGAN, Convolution, GAN",GAN,,,,,
assessing-social-and-intersectional-biases-in,"Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay, Linear Warmup With Cosine Annealing, Adam, Dropout, BPE, Linear Layer, Attention Dropout, Discriminative Fine-Tuning, GELU, Dense Connections, Cosine Annealing, Multi-Head Attention, GPT-2, WordPiece, Residual Connection",BERT,GPT,,,,
on-solving-the-2-dimensional-greedy-shooter,Q-Learning,Q-Learning,,,,,
online-gaussian-process-learning-based-model,Gaussian Process,Gaussian Process,,,,,
towards-the-use-of-neural-networks-for,GRU,GRU,,,,,
stronger-convergence-results-for-deep,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Residual Connection, Bottleneck Residual Block, ResNet, Average Pooling",ResNet,,,,,
kernel-dependence-regularizers-and-gaussian,Gaussian Process,Gaussian Process,,,,,
negbert-a-transfer-learning-approach-for,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
earthquakegen-earthquake-simulation-using,"GAN, Convolution",GAN,,,,,
subjective-sentiment-analysis-for-arabic,Logistic Regression,Logistic Regression,,,,,
preservation-of-anomalous-subgroups-on,AutoEncoder,Autoencoder,,,,,
perceiving-the-arrow-of-time-in,"LAMB, Multi-Head Attention, Adam, WordPiece, Linear Layer, Residual Connection, ALBERT, GELU, Layer Normalization, Dense Connections, Scaled Dot-Product Attention, Causal inference, Softmax",BERT,ALBERT,,,,
infra-slow-brain-dynamics-as-a-marker-for,Gaussian Process,Gaussian Process,,,,,
objectnet-a-large-scale-bias-controlled,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Bottleneck Residual Block, Residual Connection, ResNet, Average Pooling",ResNet,,,,,
icebreaker-element-wise-efficient-information,AutoEncoder,Autoencoder,,,,,
generative-adversarial-networks-gan-based,"SPEED, GAN, Convolution",GAN,,,,,
smiles-transformer-pre-trained-molecular,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
cheetah-an-ultra-fast-approximation-free-and,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout, Grouped Convolution",AlexNet,,,,,
domaingan-generating-adversarial-examples-to,"GAN, Convolution",GAN,,,,,
conjugate-gradients-for-kernel-machines,Gaussian Process,Gaussian Process,,,,,
few-features-attack-to-fool-machine-learning,"GAN, Convolution",GAN,,,,,
the-effectiveness-of-variational-autoencoders,AutoEncoder,Autoencoder,,,,,
deepsat-v2-feature-augmented-convolutional,Deep Belief Network,Deep Belief Network,,,,,
periodic-spectral-ergodicity-a-complexity,"ResNet, LSTM, Average Pooling, Dense Connections, Global Average Pooling, VGG, Residual Connection, Bottleneck Residual Block, Tanh Activation, Sigmoid Activation, Convolution, Residual Block, Softmax, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Dropout, Batch Normalization",ResNet,VGG,LSTM,,,
topological-based-classification-using-graph,GCN,GCN,,,,,
predicting-overweight-and-obesity-in-later,Logistic Regression,Logistic Regression,,,,,
mixed-curvature-variational-autoencoders-1,VAE,VAE,,,,,
deep-learning-for-epidemiological-predictions,Gaussian Process,Gaussian Process,,,,,
a-framework-for-end-to-end-deep-learning,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
machine-learning-classification-informed-by-a,SVM,SVM,,,,,
towards-reducing-bias-in-gender,AutoEncoder,Autoencoder,,,,,
rule-extraction-in-unsupervised-anomaly,SVM,SVM,,,,,
feature-extraction-based-machine-learning-for,SVM,SVM,,,,,
shape-detection-in-2d-ultrasound-images,"DPN, Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, Grouped Convolution, DPN Block, Residual Connection, Logistic Regression, Convolution, Softmax, Max Pooling, FCN, 1x1 Convolution, Batch Normalization",Logistic Regression,,,,,
factorized-multimodal-transformer-for,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
wildmix-dataset-and-spectro-temporal,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
differentially-private-federated-variational,Logistic Regression,Logistic Regression,,,,,
join-query-optimization-with-deep,Q-Learning,Q-Learning,,,,,
privacy-preserving-neural-network-inference,"Dense Connections, SPEED, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout, Grouped Convolution",AlexNet,,,,,
property-invariant-embedding-for-automated,Graph Neural Network,GNN,,,,,
sag-vae-end-to-end-joint-inference-of-data,AutoEncoder,Autoencoder,,,,,
benefits-of-jointly-training-autoencoders-an,"AutoEncoder, ReLU",Autoencoder,,,,,
solving-arithmetic-word-problems,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
a-quasi-newton-method-based-vertical,Logistic Regression,Logistic Regression,,,,,
kernelnet-a-data-dependent-kernel,AutoEncoder,Autoencoder,,,,,
make-thunderbolts-less-frightening-predicting,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, UNet++, ReLU, Kaiming Initialization, Batch Normalization, Residual Connection, Bottleneck Residual Block, ResNet, Average Pooling",U-Net++,U-Net,ResNet,,,
the-comparison-of-methods-for-individual,Logistic Regression,Logistic Regression,,,,,
learning-to-dynamically-coordinate-multi,Q-Learning,Q-Learning,,,,,
an-interpretable-prediction-model-for-obesity,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
trident-efficient-4pc-framework-for-privacy,"Linear Regression, Logistic Regression",Logistic Regression,Linear Regression,,,,
self-supervised-contextual-language,"Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer, Attention Dropout, GELU, Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, WordPiece, Residual Connection",BERT,Transformer,,,,
astra-high-throughput-3pc-over-rings-with,SVM,SVM,,,,,
privacy-preserving-inference-in-machine,"VGG-19, GAN, Convolution",GAN,VGG,VGG-19,,,
solving-forward-and-inverse-problems-using,AutoEncoder,Autoencoder,,,,,
bert-has-a-moral-compass-improvements-of,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, RoBERTa, Weight Decay",BERT,RoBERTa,,,,
variational-learning-with-disentanglement,"VAE, Beta-VAE, cVAE",VAE,,,,,
expansion-of-cyber-attack-data-from,"GAN, Convolution",GAN,,,,,
exact-expressions-for-double-descent-and,Linear Regression,Linear Regression,,,,,
peek-inside-the-closed-world-evaluating,AutoEncoder,Autoencoder,,,,,
performance-analysis-of-deep-autoencoder-and,AutoEncoder,Autoencoder,,,,,
leveraging-end-to-end-speech-recognition-with,"Tanh Activation, LSTM, Sigmoid Activation, Seq2Seq",Seq2Seq,LSTM,,,,
learning-and-optimization-with-bayesian,Gaussian Process,Gaussian Process,,,,,
long-length-legal-document-classification,"Tanh Activation, LSTM, Sigmoid Activation, BiLSTM",LSTM,BiLSTM,,,,
high-dimensional-precision-medicine-from,Q-Learning,Q-Learning,,,,,
comparisonal-study-of-deep-learning,"Average Pooling, Dense Connections, Global Average Pooling, Depthwise Separable Convolution, Depthwise Convolution, Xception, Residual Connection, Pointwise Convolution, Convolution, Softmax, Max Pooling, MobileNetV2, 1x1 Convolution, ReLU, Inverted Residual Block, Batch Normalization",Xception,LeNet,MobileNetV2,,,
a-robust-predictive-model-for-stock-price,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
sigma-strengthening-ids-with-gan-and,"GAN, Convolution",GAN,,,,,
deep-mining-detecting-anomalous-patterns-in,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Residual Connection, Bottleneck Residual Block, ResNet, Average Pooling",ResNet,,,,,
learning-neural-surrogate-model-for-warm,Gaussian Process,Gaussian Process,,,,,
a-fair-comparison-of-graph-neural-networks-1,"Graph Neural Network, RAN",GNN,,,,,
empirical-comparisons-of-cnn-with-other,Logistic Regression,Logistic Regression,,,,,
latent-variables-on-spheres-for-sampling-and-1,"AutoEncoder, VAE",VAE,Autoencoder,,,,
random-capsnet-forest-model-for-imbalanced,Capsule Network,Capsule Network,,,,,
tensor-basis-gaussian-process-models-of,Gaussian Process,Gaussian Process,,,,,
customers-churn-prediction-in-financial,Logistic Regression,Logistic Regression,,,,,
betanas-balanced-training-and-selective-drop-1,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
pruning-deep-neural-networks-architectures,"DCNN, Convolution, Pruning",CNN,DCNN,,,,
a-dynamic-sampling-adaptive-sgd-method-for,"Logistic Regression, SGD, Adam",Logistic Regression,,,,,
autodiscern-rating-the-quality-of-online,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
natural-language-processing-of-mimic-iii,"Weight Tying, ULMFiT, Dropout, Temporal Activation Regularization, AWD-LSTM, Variational Dropout, Discriminative Fine-Tuning, Tanh Activation, Embedding Dropout, DropConnect, LSTM, Sigmoid Activation, Activation Regularization, Slanted Triangular Learning Rates",LSTM,,,,,
text-classification-for-azerbaijani-language,SVM,SVM,,,,,
kernelized-support-tensor-train-machines,SVM,SVM,,,,,
pre-trained-contextual-embedding-of-source-1,"Layer Normalization, Sigmoid Activation, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay, Position-Wise Feed-Forward Layer, Adam, Absolute Position Encodings, Dropout, BPE, Linear Layer, Attention Dropout, GELU, LSTM, Dense Connections, Label Smoothing, Transformer, BiLSTM, Multi-Head Attention, CuBERT, WordPiece, Residual Connection, Tanh Activation",BERT,Transformer,LSTM,BiLSTM,,
visual-evaluation-of-generative-adversarial,"GAN, Convolution",GAN,,,,,
forecasting-bitcoin-closing-price-series,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
identifying-and-compensating-for-feature,Logistic Regression,Logistic Regression,,,,,
simulation-of-turbulent-flow-around-a-generic,Gaussian Process,Gaussian Process,,,,,
exploiting-event-driven-cameras-for-spatio,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
machine-learning-classifiers-for-logographic,Logistic Regression,Logistic Regression,,,,,
macromolecule-classification-based-on-the,"Tanh Activation, GRU, LSTM, Sigmoid Activation",GRU,LSTM,,,,
choosing-the-sample-with-lowest-loss-makes,"Linear Regression, SGD",Linear Regression,,,,,
multi-layer-optimizations-for-end-to-end-data,Linear Regression,Linear Regression,,,,,
an-explainable-autoencoder-for-collaborative,AutoEncoder,Autoencoder,,,,,
ddsp-differentiable-digital-signal-processing-1,"GRU, DDSP",GRU,,,,,
unsupervised-pool-based-active-learning-for,Linear Regression,Linear Regression,,,,,
machine-learning-for-total-cloud-cover,Logistic Regression,Logistic Regression,,,,,
scalable-hyperparameter-optimization-with-1,"Gaussian Process, SPEED",Gaussian Process,,,,,
graphbgs-background-subtraction-via-recovery,"Mask R-CNN, RPN, RoIAlign, Convolution, Softmax",CNN,,,,,
data-driven-permanent-magnet-temperature,Linear Regression,Linear Regression,,,,,
sglb-stochastic-gradient-langevin-boosting,Logistic Regression,Logistic Regression,,,,,
up-to-two-billion-times-acceleration-of,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
expected-information-maximization-using-the-i-1,"GAN, Convolution",GAN,,,,,
sentiment-polarity-detection-in-azerbaijani,SVM,SVM,,,,,
predicting-sentiment-of-polish-language-short,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, LSTM, Dense Connections, ELMo, BiLSTM, Multi-Head Attention, WordPiece, Residual Connection, Tanh Activation, Layer Normalization, Sigmoid Activation, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,ELMo,LSTM,BiLSTM,,
turkish-tweet-classification-with-transformer,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
machine-learning-for-a-music-glove-instrument,"Logistic Regression, GloVe",Logistic Regression,,,,,
imperfect-imaganation-implications-of-gans,"ProGAN, Leaky ReLU, Dense Connections, GAN, Convolution, Local Response Normalization, WGAN-GP Loss, 1x1 Convolution",GAN,,,,,
opfython-a-python-inspired-optimum-path,Logistic Regression,Logistic Regression,,,,,
convergence-guarantees-for-gaussian-process,Gaussian Process,Gaussian Process,,,,,
privacy-preserving-gaussian-process,Gaussian Process,Gaussian Process,,,,,
multi-participant-multi-class-vertical,"Logistic Regression, Feature Selection",Logistic Regression,,,,,
a-hybrid-quantum-enabled-rbm-advantage,"AutoEncoder, Restricted Boltzmann Machine",Autoencoder,,,,,
deep-learning-based-unsupervised-and-semi,AutoEncoder,Autoencoder,,,,,
autofcl-automatically-tuning-fully-connected,"ResNet, Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, DenseNet, Residual Connection, Bottleneck Residual Block, Softmax, Residual Block, Convolution, Max Pooling, 1x1 Convolution, Dense Block, ReLU, Dropout, Kaiming Initialization, Batch Normalization",ResNet,DenseNet,,,,
data-driven-discovery-of-coarse-grained,Linear Regression,Linear Regression,,,,,
weatherbench-a-benchmark-dataset-for-data,Linear Regression,Linear Regression,,,,,
linearly-constrained-gaussian-processes-with,Gaussian Process,Gaussian Process,,,,,
improved-subsampled-randomized-hadamard,SVM,SVM,,,,,
macroscopic-traffic-flow-modeling-with,Gaussian Process,Gaussian Process,,,,,
the-costs-and-benefits-of-goal-directed,DCNN,CNN,DCNN,,,,
perm2vec-graph-permutation-selection-for,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
mitigating-query-flooding-parameter,Linear Regression,Linear Regression,,,,,
defending-adversarial-attacks-via-semantic,AutoEncoder,Autoencoder,,,,,
understanding-and-optimizing-packed-neural,"Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, DenseNet, Softmax, Convolution, Max Pooling, 1x1 Convolution, Dense Block, ReLU, Dropout, Kaiming Initialization, Batch Normalization",DenseNet,,,,,
the-effect-of-data-augmentation-on,"GAN, Convolution",GAN,,,,,
high-temporal-resolution-rainfall-runoff,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
one-shot-bayes-opt-with-probabilistic,"Gaussian Process, Population Based Training",Gaussian Process,,,,,
nonlinear-equation-solving-a-faster,PixelCNN,CNN,,,,,
a-spike-in-performance-training-hybrid,"Tanh Activation, GRU, LSTM, Sigmoid Activation",GRU,LSTM,,,,
a-physiology-driven-computational-model-for,Logistic Regression,Logistic Regression,,,,,
on-a-scalable-entropic-breaching-of-the,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
finitenet-a-fully-convolutional-lstm-network,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
deep-transfer-learning-for-physiological,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
more-data-can-expand-the-generalization-gap,Linear Regression,Linear Regression,,,,,
superbloom-bloom-filter-meets-transformer-1,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
image-analysis-enhanced-event-detection-from,AutoEncoder,Autoencoder,,,,,
graph-prolongation-convolutional-networks,GCN,GCN,,,,,
minimax-theorem-for-latent-games-or-how-i,"WGAN, Convolution",GAN,,,,,
how-to-0wn-nas-in-your-spare-time,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
active-bayesian-assessment-for-black-box,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Residual Connection, Bottleneck Residual Block, ResNet, Average Pooling",ResNet,,,,,
tplvm-portfolio-construction-by-students-t,Gaussian Process,Gaussian Process,,,,,
application-of-pre-training-models-in-named,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, RoBERTa, ERNIE, Weight Decay",BERT,RoBERTa,,,,
comparative-visual-analytics-for-assessing,AutoEncoder,Autoencoder,,,,,
adversarial-detection-and-correction-by,AutoEncoder,Autoencoder,,,,,
efficient-learning-of-model-weights-via,Linear Regression,Linear Regression,,,,,
maxup-a-simple-way-to-improve-generalization,"DropConnect, CutMix, Average Pooling, Dense Connections, Global Average Pooling, ProxylessNet-GPU, Residual Connection, Bottleneck Residual Block, Embedding Dropout, NT-ASGD, Sigmoid Activation, FixRes, Squeeze-and-Excitation Block, ProxylessNet-Mobile, Pointwise Convolution, Weight Decay, Weight Tying, 1x1 Convolution, Dropout, Random Resized Crop, Variational Dropout, ResNet, LSTM, Random Horizontal Flip, EfficientNet, Depthwise Separable Convolution, RMSProp, Depthwise Convolution, Temporal Activation Regularization, MaxUp, AWD-LSTM, Tanh Activation, Activation Regularization, ProxylessNet-CPU, Convolution, Residual Block, Max Pooling, ReLU, Inverted Residual Block, Swish, Kaiming Initialization, Batch Normalization",ResNet,EfficientNet,LSTM,,,
forecasting-foreign-exchange-rate-a,SVM,SVM,,,,,
empirical-study-on-airline-delay-analysis-and,Logistic Regression,Logistic Regression,,,,,
freeze-discriminator-a-simple-baseline-for,"Feedforward Network, Leaky ReLU, Dense Connections, Adaptive Instance Normalization, GAN, Convolution, R1 Regularization, StyleGAN",GAN,StyleGAN,,,,
counterfactual-fairness-removing-direct,Logistic Regression,Logistic Regression,,,,,
graphcore-c2-card-performance-for-image-based,"Global Average Pooling, Convolution, 1x1 Convolution, ResNeXt, ResNeXt Block, ReLU, Grouped Convolution, Kaiming Initialization, Batch Normalization, Residual Connection, Average Pooling",ResNeXt,,,,,
nestedvae-isolating-common-factors-via-weak,"AutoEncoder, VAE",VAE,Autoencoder,,,,
supervised-dimensionality-reduction-and,"PCA, AutoEncoder",Autoencoder,,,,,
imbalance-learning-for-variable-star,Gaussian Process,Gaussian Process,,,,,
attention-aware-fusion-rgb-d-face-recognition,"Tanh Activation, LSTM, Sigmoid Activation, Convolution",LSTM,,,,,
introducing-fuzzy-layers-for-deep-learning,"GoogLeNet, Average Pooling, Dense Connections, AlexNet, Label Smoothing, Inception Module, RMSProp, Inception-v3 Module, Grouped Convolution, Auxiliary Classifier, Inception-v3, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout",GoogleNet,LeNet,AlexNet,,,
an-information-theoretic-approach-to-3,Linear Regression,Linear Regression,,,,,
tensor-networks-for-language-modeling,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
disease-detection-from-lung-x-ray-images,"Capsule Network, Dense Connections, Spatial Transformer, Softmax, Convolution, VGG, Max Pooling, ReLU, Dropout, STN",Transformer,VGG,Capsule Network,,,
automatic-differentiation-variational-1,AutoEncoder,Autoencoder,,,,,
amateur-drones-detection-a-machine-learning,"SVM, ICA",SVM,,,,,
real-time-federated-evolutionary-neural,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
cross-modal-learning-for-multi-modal-video,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, ReLU, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,,,,,
puminer-mining-security-posts-from-developer,SVM,SVM,,,,,
fedloc-federated-learning-framework-for,Gaussian Process,Gaussian Process,,,,,
security-of-distributed-machine-learning-a,SVM,SVM,,,,,
a-precisely-xtreme-multi-channel-hybrid,Logistic Regression,Logistic Regression,,,,,
short-term-forecasting-of-co2-emission,"Linear Regression, Softmax, Feature Selection",Linear Regression,,,,,
application-of-deep-q-network-in-portfolio,"DQN, Dense Connections, Q-Learning, Convolution",Q-Learning,,,,,
human-activity-recognition-using-multi-head,"Tanh Activation, LSTM, Sigmoid Activation, SVM",SVM,LSTM,,,,
pool-based-unsupervised-active-learning-for,Linear Regression,Linear Regression,,,,,
radiomic-feature-selection-for-lung-cancer,"SVM, Feature Selection",SVM,,,,,
improving-predictions-by-nonlinear-regression,Linear Regression,Linear Regression,,,,,
uncertainty-estimation-in-cancer-survival,Logistic Regression,Logistic Regression,,,,,
mirrored-autoencoders-with-simplex,AutoEncoder,Autoencoder,,,,,
preferential-batch-bayesian-optimization,Gaussian Process,Gaussian Process,,,,,
how-deep-is-your-encoder-an-analysis-of,AutoEncoder,Autoencoder,,,,,
obliviousness-makes-poisoning-adversaries,"Linear Regression, Feature Selection",Linear Regression,,,,,
pipelined-backpropagation-at-scale-training,"ResNet, Average Pooling, Dense Connections, SPEED, Global Average Pooling, VGG, Pipelined Backpropagation, Bottleneck Residual Block, Residual Connection, SGD, Online Normalization, Convolution, Residual Block, Softmax, Max Pooling, Group Normalization, 1x1 Convolution, ReLU, Kaiming Initialization, Dropout, Batch Normalization",GAT,VGG,ResNet,,,
tracer-a-framework-for-facilitating-accurate,Logistic Regression,Logistic Regression,,,,,
introduction-to-rare-event-predictive,AutoEncoder,Autoencoder,,,,,
latent-patient-network-learning-for-automatic,GCN,GCN,,,,,
global-attention-based-graph-convolutional,"Graph Neural Network, GAT",GAT,GNN,,,,
a-hierarchical-transformer-for-unsupervised,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
semantic-based-end-to-end-learning-for,"Tanh Activation, LSTM, Sigmoid Activation, BiLSTM",LSTM,BiLSTM,,,,
digit-recognition-using-convolution-neural,"Convolution, SVM",SVM,,,,,
botnet-detection-using-recurrent-variational,AutoEncoder,Autoencoder,,,,,
nbdt-neural-backed-decision-trees,"Average Pooling, Dense Connections, EfficientNet, Depthwise Separable Convolution, RMSProp, Depthwise Convolution, Sigmoid Activation, Squeeze-and-Excitation Block, Pointwise Convolution, Convolution, 1x1 Convolution, ReLU, Dropout, Swish, Inverted Residual Block, Batch Normalization, Linear Layer",EfficientNet,,,,,
a-hybrid-residual-dilated-lstm-end,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
multi-class-classification-of-vulnerabilities,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
deep-learning-for-stock-market-prediction,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
improved-code-summarization-via-a-graph,Graph Neural Network,GNN,,,,,
leveraging-the-inherent-hierarchy-of-vacancy,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
falcon-honest-majority-maliciously-secure,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dropout, Grouped Convolution, Batch Normalization",AlexNet,,,,,
using-machine-learning-approach-for,Linear Regression,Linear Regression,,,,,
mapping-individual-differences-in-cortical,AutoEncoder,Autoencoder,,,,,
computer-vision-and-abnormal-patient-gait,SVM,SVM,,,,,
increasing-the-inference-and-learning-speed,Logistic Regression,Logistic Regression,,,,,
deepstreamce-a-streaming-approach-to-concept,AutoEncoder,Autoencoder,,,,,
file-classification-based-on-spiking-neural,Logistic Regression,Logistic Regression,,,,,
orthant-based-proximal-stochastic-gradient,"Average Pooling, Dense Connections, Global Average Pooling, Pointwise Convolution, MobileNetV1, Convolution, Softmax, Depthwise Separable Convolution, 1x1 Convolution, ReLU, Depthwise Convolution, Feature Selection, Batch Normalization",LeNet,MobileNetV1,,,,
cnn2gate-toward-designing-a-general-framework,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, VGG, 1x1 Convolution, ReLU, Dropout, Grouped Convolution",VGG,AlexNet,,,,
performance-accuration-method-of-machine,"Logistic Regression, k-NN",Logistic Regression,,,,,
data-augmentation-using-generative-networks,AutoEncoder,Autoencoder,,,,,
scenario-optimization-with-relaxation-a-new,SVM,SVM,,,,,
joint-user-pairing-and-association-for,"Pointer Network, Tanh Activation, LSTM, Sigmoid Activation, Additive Attention, Softmax",LSTM,,,,,
asymmetrically-vertical-federated-learning,Logistic Regression,Logistic Regression,,,,,
machine-learning-for-multiple-yield-curve,"Gaussian Process, Adam",Gaussian Process,,,,,
machine-learning-prediction-errors-better,Linear Regression,Linear Regression,,,,,
gevo-gpu-code-optimization-using,SVM,SVM,,,,,
recommendation-system-using-a-deep-learning,"AutoEncoder, node2vec",Autoencoder,,,,,
yurugan-yuru-chara-mascot-generator-using,"GAN, Convolution",GAN,,,,,
counterfactual-confounding-adjustment-for,"Logistic Regression, Softmax",Logistic Regression,,,,,
adversarial-training-for-large-neural,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, RoBERTa, Weight Decay",BERT,RoBERTa,,,,
lightweight-mask-r-cnn-for-long-range,"Mask R-CNN, RPN, RoIAlign, Convolution, Softmax",CNN,,,,,
learning-to-rank-with-bert-in-tf-ranking,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, ELECTRA, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, RoBERTa, Weight Decay",BERT,ELECTRA,RoBERTa,,,
a-mathematical-programming-approach-to-binary,SVM,SVM,,,,,
a-novel-embedded-min-max-approach-for-feature,"SVM, Feature Selection",SVM,,,,,
predicting-nucleation-near-the-spinodal-in,Logistic Regression,Logistic Regression,,,,,
dependency-based-neural-representations-for,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
deepbedmap-using-a-deep-neural-network-to,GAN,GAN,,,,,
deeply-uncertain-comparing-methods-of,"Deep Ensembles, Dropout, Concrete Dropout",Ensemble,,,,,
automatic-detection-of-coronavirus-disease-1,"ResNet, Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, DenseNet, Depthwise Separable Convolution, Depthwise Convolution, Xception, Residual Connection, Bottleneck Residual Block, Pointwise Convolution, Softmax, Residual Block, Convolution, Max Pooling, 1x1 Convolution, ReLU, Dense Block, Dropout, Kaiming Initialization, Batch Normalization",Xception,ResNet,DenseNet,,,
normalizing-flow-regression,Linear Regression,Linear Regression,,,,,
active-learning-for-gaussian-process,"Gaussian Process, Linear Regression",Linear Regression,Gaussian Process,,,,
statistical-characterization-and,"Gaussian Process, Logistic Regression",Logistic Regression,Gaussian Process,,,,
learning-decision-ensemble-using-a-graph,Graph Neural Network,GNN,,,,,
a-systematic-search-over-deep-convolutional,"Residual Connection, Average Pooling, Dense Connections, Global Average Pooling, Pointwise Convolution, Convolution, Softmax, Max Pooling, Depthwise Separable Convolution, 1x1 Convolution, ReLU, Depthwise Convolution, Xception",Xception,,,,,
improvement-in-land-cover-and-crop,"R-CNN, Convolution, Max Pooling, SVM",SVM,CNN,,,,
evolving-inborn-knowledge-for-fast-adaptation,"SPEED, AutoEncoder",Autoencoder,,,,,
colbert-using-bert-sentence-embedding-for,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
augmenting-transformers-with-knn-based-1,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, ReLU, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
hybrid-neuro-evolutionary-method-for,"AutoEncoder, k-Means Clustering",Autoencoder,,,,,
scelmo-source-code-embeddings-from-language-1,"Tanh Activation, LSTM, Sigmoid Activation, ELMo, Softmax, BiLSTM",ELMo,LSTM,BiLSTM,,,
autoencoding-neural-networks-as-musical-audio,AutoEncoder,Autoencoder,,,,,
the-impact-of-the-mini-batch-size-on-the,"Linear Regression, SGD",Linear Regression,,,,,
netml-a-challenge-for-network-traffic,SVM,SVM,,,,,
development-of-a-machine-learning-system-to,SVM,SVM,,,,,
investigating-the-effectiveness-of,"Adam, Dropout, Linear Layer, Attention Dropout, ALBERT, GELU, Dense Connections, LAMB, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,ALBERT,,,,
neural-additive-models-interpretable-machine,"Logistic Regression, NAM",Logistic Regression,,,,,
a-natural-language-processing-pipeline-of,SVM,SVM,,,,,
sentiment-analysis-of-yelp-reviews-a,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, LSTM, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Tanh Activation, Logistic Regression, Layer Normalization, Sigmoid Activation, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,Logistic Regression,LSTM,,,
skipgnn-predicting-molecular-interactions,Graph Neural Network,GNN,,,,,
binary-autoencoder-with-random-binary-weights,AutoEncoder,Autoencoder,,,,,
generalization-error-of-generalized-linear,Linear Regression,Linear Regression,,,,,
lupulus-a-flexible-hardware-accelerator-for,"Dense Connections, AlexNet, Softmax, Local Response Normalization, Convolution, Max Pooling, VGG, 1x1 Convolution, ReLU, Dropout, Grouped Convolution",VGG,AlexNet,,,,
explaining-how-deep-neural-networks-forget-by,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Residual Connection, Bottleneck Residual Block, ResNet, Average Pooling",ResNet,,,,,
deep-convolutional-neural-networks-to,"Residual Connection, Average Pooling, Dense Connections, Global Average Pooling, Pointwise Convolution, Softmax, Convolution, Max Pooling, Depthwise Separable Convolution, 1x1 Convolution, Adam, ReLU, Depthwise Convolution, Xception",Xception,,,,,
data-augmentation-via-mixed-class,"Convolution, Residual Block, Cycle Consistency Loss, GAN Least Squares Loss, ReLU, Batch Normalization, Tanh Activation, Residual Connection, CycleGAN, Mixup, Sigmoid Activation, Leaky ReLU, PatchGAN, Instance Normalization",GAN,,,,,
long-short-term-memory-networks-and-laglasso,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
using-machine-learning-to-emulate-agent-based,Gaussian Process,Gaussian Process,,,,,
automatic-plant-image-identification-of,"ResNet, Average Pooling, Global Average Pooling, Depthwise Separable Convolution, Depthwise Convolution, Residual Connection, Bottleneck Residual Block, Pointwise Convolution, Convolution, Residual Block, Max Pooling, MobileNetV2, 1x1 Convolution, ReLU, Kaiming Initialization, Inverted Residual Block, Batch Normalization",ResNet,LeNet,MobileNetV2,,,
one-step-regression-and-classification-with,"Linear Regression, Logistic Regression",Logistic Regression,Linear Regression,,,,
interpreting-rate-distortion-of-variational,"AutoEncoder, VAE",VAE,Autoencoder,,,,
estimating-blood-pressure-from,"Gaussian Process, Feature Selection",Gaussian Process,,,,,
a-locally-adaptive-interpretable-regression,Linear Regression,Linear Regression,,,,,
covid-19-growth-prediction-using-multivariate,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
autoclint-the-winning-method-in-autocv,"Tanh Activation, LSTM, Sigmoid Activation, AutoAugment, Fast AutoAugment",LSTM,,,,,
machine-learning-based-digital-twin-for,Gaussian Process,Gaussian Process,,,,,
perturbing-inputs-to-prevent-model-stealing,Logistic Regression,Logistic Regression,,,,,
segmenting-scientific-abstracts-into,"Tanh Activation, LSTM, CRF, Sigmoid Activation, LSH Attention, GloVe, BiLSTM",LSTM,BiLSTM,,,,
phishing-url-detection-through-top-level,SVM,SVM,,,,,
cross-lingual-transfer-of-twitter-sentiment,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
when-to-lift-the-lockdown-global-covid-19,Gaussian Process,Gaussian Process,,,,,
blaze-blazing-fast-privacy-preserving-machine,"Linear Regression, Logistic Regression",Logistic Regression,Linear Regression,,,,
should-we-hard-code-the-recurrence-concept-or,"LSTM, Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Tanh Activation, Residual Connection, Layer Normalization, Sigmoid Activation, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, ReLU, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,LSTM,,,,
an-lstm-approach-to-predict-migration-based,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
learning-semantic-program-embeddings-with,Graph Neural Network,GNN,,,,,
unsupposable-test-data-generation-for-machine,"AutoEncoder, VAE",VAE,Autoencoder,,,,
swift-super-fast-and-robust-privacy,Logistic Regression,Logistic Regression,,,,,
comparative-study-of-machine-learning-models,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
on-compression-rate-of-quantum-autoencoders,AutoEncoder,Autoencoder,,,,,
global-optimization-of-gaussian-processes,Gaussian Process,Gaussian Process,,,,,
covid-19-public-sentiment-insights-and,Logistic Regression,Logistic Regression,,,,,
mango-a-python-library-for-parallel,"Gaussian Process, Random Search, Tree-structured Parzen Estimator Approach (TPE)",Gaussian Process,,,,,
consistency-of-empirical-bayes-and-kernel,Gaussian Process,Gaussian Process,,,,,
analyzing-elmo-and-distilbert-on-socio,"Adam, Dropout, Linear Layer, Attention Dropout, Memory Network, GELU, LSTM, Dense Connections, ELMo, BiLSTM, Multi-Head Attention, DistilBERT, WordPiece, Tanh Activation, Residual Connection, Sigmoid Activation, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,ELMo,LSTM,BiLSTM,,
implementation-of-supervised-training,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
detecting-direct-speech-in-multilingual,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, ReLU, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,,,,,
asu-opto-at-osact4-offensive-language,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, ReLU, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,,,,,
understanding-user-utterances-in-a-dialog,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
decop-a-multilingual-and-multi-domain-corpus,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, ReLU, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,,,,,
abusive-language-in-spanish-children-and,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
clfd-a-novel-vectorization-technique-and-its,Logistic Regression,Logistic Regression,,,,,
comparing-methods-for-measuring-dialect,AutoEncoder,Autoencoder,,,,,
evaluation-of-manual-and-non-manual,Logistic Regression,Logistic Regression,,,,,
path-imputation-strategies-for-signature,Gaussian Process,Gaussian Process,,,,,
causalm-causal-model-explanation-through,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
safeml-safety-monitoring-of-machine-learning,"Gaussian Process, Feedforward Network, Dense Connections, SVM",SVM,Gaussian Process,,,,
semi-supervised-source-localization-with-deep,VAE,VAE,,,,,
comparing-bert-against-traditional-machine,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
codebook-based-beam-tracking-for-conformal,Gaussian Process,Gaussian Process,,,,,
network-fusion-for-content-creation-with,"Batch Normalization, Attention Dropout, GELU, GAN Hinge Loss, Spectral Normalization, Dense Connections, WordPiece, Residual Connection, Non-Local Block, Layer Normalization, Scaled Dot-Product Attention, Softmax, Weight Decay, 1x1 Convolution, Dropout, Linear Layer, Truncation Trick, BigGAN, Projection Discriminator, Multi-Head Attention, Feedforward Network, TTUR, Linear Warmup With Linear Decay, Conditional Batch Normalization, BERT, Early Stopping, SAGAN Self-Attention Module, SAGAN, Dot-Product Attention, Residual Block, Convolution, Non-Local Operation, Off-Diagonal Orthogonal Regularization, Adam, ReLU",BERT,GAN,BigGAN,,,
using-machine-learning-to-forecast-future,Logistic Regression,Logistic Regression,,,,,
review-of-mathematical-frameworks-for,Linear Regression,Linear Regression,,,,,
detection-of-bangla-fake-news-using-mnb-and,SVM,SVM,,,,,
learning-how-to-learn-within-an-lsm-based-key,Linear Regression,Linear Regression,,,,,
using-generative-models-for-pediatric-wbmri,"Leaky ReLU, Path Length Regularization, Convolution, StyleGAN2, R1 Regularization, Weight Demodulation",GAN,StyleGAN2,StyleGAN,,,
adahessian-an-adaptive-second-order-optimizer,"Average Pooling, Dense Connections, Global Average Pooling, Label Smoothing, Bottleneck Residual Block, Residual Connection, AdaHessian, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, 1x1 Convolution, AdamW, Dropout, Linear Layer, ResNet, Transformer, Multi-Head Attention, AdaGrad, SGD, Convolution, Residual Block, Max Pooling, Adam, ReLU, Absolute Position Encodings, Kaiming Initialization, BPE, Batch Normalization",Transformer,ResNet,,,,
high-fidelity-audio-generation-and,AutoEncoder,Autoencoder,,,,,
a-hierarchical-deep-convolutional-neural,"Memory Network, GRU",GRU,,,,,
a2-link-recognizing-disguised-faces-via,"Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, DenseNet, ArcFace, Convolution, Softmax, Max Pooling, 1x1 Convolution, Dense Block, ReLU, Dropout, Kaiming Initialization, Batch Normalization",DenseNet,,,,,
a-comparative-study-of-2d-image-segmentation,UNet++,U-Net++,U-Net,,,,
resolving-class-imbalance-in-object-detection,"RPN, RoIPool, Convolution, Softmax, Faster R-CNN, Focal Loss",CNN,Faster R-CNN,,,,
transfer-learning-for-british-sign-language-1,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
automatic-text-summarization-of-covid-19,"Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay, Linear Warmup With Cosine Annealing, Adam, BPE, Dropout, Linear Layer, Discriminative Fine-Tuning, Attention Dropout, GELU, Dense Connections, Cosine Annealing, Multi-Head Attention, GPT-2, WordPiece, Residual Connection",BERT,GPT,,,,
variational-mutual-information-maximization,"AutoEncoder, VAE",VAE,Autoencoder,,,,
a-novel-update-mechanism-for-q-networks-based,"SPEED, Q-Learning",Q-Learning,,,,,
semi-supervised-and-unsupervised-methods-for-1,"AutoEncoder, SVM",SVM,Autoencoder,,,,
learning-to-rank-learning-curves-1,"Tanh Activation, LSTM, Sigmoid Activation, Softmax",LSTM,,,,,
inject-machine-learning-into-significance,Linear Regression,Linear Regression,,,,,
predicting-molecular-dipole-moments-by,Gaussian Process,Gaussian Process,,,,,
a-geometric-look-at-double-descent-risk,Linear Regression,Linear Regression,,,,,
health-indicator-forecasting-for-improving,Gaussian Process,Gaussian Process,,,,,
sparse-gaussian-processes-via-parametric,Gaussian Process,Gaussian Process,,,,,
an-overview-of-neural-network-compression,"Knowledge Distillation, Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, ReLU, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,,,,,
detecting-problem-statements-in-peer,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Logistic Regression, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, GloVe, Softmax, Weight Decay",BERT,Logistic Regression,,,,
bert-xml-large-scale-automated-icd-coding,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
coresets-for-near-convex-functions,"Coresets, SVM",SVM,,,,,
fast-modeling-and-understanding-fluid,"Tanh Activation, LSTM, Sigmoid Activation, SPEED, AutoEncoder",LSTM,Autoencoder,,,,
ar-dae-towards-unbiased-neural-entropy,"Denoising Autoencoder, AutoEncoder",Autoencoder,,,,,
automated-quantification-of-ct-patterns,Logistic Regression,Logistic Regression,,,,,
interpretable-signal-analysis-with-knockoffs,Logistic Regression,Logistic Regression,,,,,
a-comparative-study-on-early-detection-of,"Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, CheXNet, DenseNet, Convolution, Softmax, Max Pooling, 1x1 Convolution, Dense Block, ReLU, Dropout, Kaiming Initialization, Batch Normalization",CheXNet,DenseNet,,,,
data-augmentation-for-enhancing-eeg-based,"WGAN, AutoEncoder, VAE, Convolution",GAN,VAE,Autoencoder,,,
multiplicative-noise-and-heavy-tails-in,"Linear Regression, SGD, Adam",Linear Regression,,,,,
machine-learning-model-to-cluster-and-map,SVM,SVM,,,,,
dance-revolution-long-sequence-dance,"LSTM, Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Tanh Activation, Layer Normalization, Sigmoid Activation, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, ReLU, Adam, Seq2Seq, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,Seq2Seq,LSTM,,,
fourier-sparse-leverage-scores-and,Gaussian Process,Gaussian Process,,,,,
learning-effective-representations-for-person,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
interaction-networks-using-a-reinforcement,"DQN, Dense Connections, Q-Learning, Convolution",Q-Learning,,,,,
societal-biases-reinforcement-through-machine,SVM,SVM,,,,,
robust-variational-autoencoder-for-tabular,AutoEncoder,Autoencoder,,,,,
explaining-predictions-by-approximating-the,AutoEncoder,Autoencoder,,,,,
sub-seasonal-climate-forecasting-via-machine,Linear Regression,Linear Regression,,,,,
gp3-a-sampling-based-analysis-framework-for,Gaussian Process,Gaussian Process,,,,,
defending-support-vector-machines-against,SVM,SVM,,,,,
high-dimensional-similarity-search-with,"AutoEncoder, VAE",VAE,Autoencoder,,,,
subjective-question-answering-deciphering-the,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, ReLU, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,,,,,
a-survey-of-constrained-gaussian-process,Gaussian Process,Gaussian Process,,,,,
unified-svm-algorithm-based-ls-dc-loss,SVM,SVM,,,,,
relational-fusion-networks-graph,GCN,GCN,,,,,
learning-to-map-source-code-to-software,Graph Neural Network,GNN,,,,,
modelling-high-level-mathematical-reasoning,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, ReLU, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,,,,,
comparative-sentiment-analysis-of-app-reviews,SVM,SVM,,,,,
toward-theory-of-applied-learning-what-is,"k-NN, SVM",SVM,,,,,
sarcasm-identification-and-detection-in,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
automatic-extraction-of-personal-events-from,SVM,SVM,,,,,
mapping-of-narrative-text-fields-to-icd-10,Logistic Regression,Logistic Regression,,,,,
sarcasm-detection-in-tweets-with-bert-and-1,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, GloVe, Softmax, Weight Decay",BERT,,,,,
deep-belief-network-based-representation,Graph Neural Network,GNN,,,,,
statistical-mechanics-of-generalization-in,Linear Regression,Linear Regression,,,,,
the-macroeconomy-as-a-random-forest,Linear Regression,Linear Regression,,,,,
distributionally-robust-machine-learning,Linear Regression,Linear Regression,,,,,
active-online-domain-adaptation,Linear Regression,Linear Regression,,,,,
green-machine-learning-via-augmented-gaussian,"Gaussian Process, SVM",SVM,Gaussian Process,,,,
on-the-difficulty-of-designing-processor,"ResNet, Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, DenseNet, Residual Connection, Bottleneck Residual Block, Residual Block, Convolution, Softmax, Max Pooling, 1x1 Convolution, Dense Block, ReLU, Kaiming Initialization, Dropout, Batch Normalization",ResNet,DenseNet,,,,
real-time-prediction-of-bitcoin-price-using,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
validating-psychometric-survey-responses,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
predicting-customer-churn-in-world-of,Logistic Regression,Logistic Regression,,,,,
conditional-set-generation-with-transformers,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, ReLU, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,,,,,
do-not-forget-interaction-predicting-fatality,Logistic Regression,Logistic Regression,,,,,
gshard-scaling-giant-models-with-conditional,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, GShard, Position-Wise Feed-Forward Layer, ReLU, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,,,,,
data-movement-is-all-you-need-a-case-study-of,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
bidirectional-encoder-representations-from,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, LSTM, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Tanh Activation, Layer Normalization, Sigmoid Activation, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,LSTM,,,,
high-dimensional-bayesian-optimization,"Gaussian Process, PCA",Gaussian Process,,,,,
can-we-achieve-more-with-less-exploring-data,"Memory Network, Logistic Regression",Logistic Regression,,,,,
gaussian-process-regression-with-local,Gaussian Process,Gaussian Process,,,,,
efficient-computation-and-analysis-of,Linear Regression,Linear Regression,,,,,
coded-distributed-computing-with-partial,"Linear Regression, SPEED",Linear Regression,,,,,
estimation-of-ground-contacts-from-human-gait,SVM,SVM,,,,,
sentiment-polarity-detection-on-bengali-book,SVM,SVM,,,,,
blockflow-an-accountable-and-privacy,Logistic Regression,Logistic Regression,,,,,
cooking-is-all-about-people-comment,"Weight Decay, Adam, BPE, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, XLM, Multi-Head Attention, WordPiece, DistilBERT, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax",BERT,XLM,,,,
a-survey-of-real-time-social-based-traffic,SVM,SVM,,,,,
winning-with-simple-learning-models-detecting,Logistic Regression,Logistic Regression,,,,,
understanding-the-effect-of-hyperparameter,"Gaussian Process, SVM",SVM,Gaussian Process,,,,
an-interpretable-baseline-for-time-series,SVM,SVM,,,,,
mts-cyclegan-an-adversarial-based-deep,"Cycle Consistency Loss, ReLU, GAN Least Squares Loss, Batch Normalization, Tanh Activation, Residual Connection, CycleGAN, Sigmoid Activation, PatchGAN, Leaky ReLU, AutoEncoder, Instance Normalization, Convolution, Residual Block",GAN,Autoencoder,,,,
highway-traffic-state-estimation-using,Gaussian Process,Gaussian Process,,,,,
numerical-simulation-clustering-and,"Gaussian Process, AutoEncoder",Autoencoder,Gaussian Process,,,,
learning-perturbation-sets-for-robust-machine,AutoEncoder,Autoencoder,,,,,
cloudcast-a-satellite-based-dataset-and,"Tanh Activation, Sigmoid Activation, ConvLSTM, Convolution",LSTM,ConvLSTM,,,,
in-search-of-the-weirdest-galaxies-in-the,AutoEncoder,Autoencoder,,,,,
graph-neural-networks-for-node-level,Graph Neural Network,GNN,,,,,
mcunet-tiny-deep-learning-on-iot-devices,"Average Pooling, Pointwise Convolution, Convolution, Depthwise Separable Convolution, MobileNetV2, 1x1 Convolution, Depthwise Convolution, Inverted Residual Block, Batch Normalization",LeNet,MobileNetV2,,,,
revisiting-sorted-table-search-procedures,Linear Regression,Linear Regression,,,,,
a-machine-learning-approach-for-task-and,Q-Learning,Q-Learning,,,,,
unsupervised-anomaly-detection-for-discrete,"Tanh Activation, LSTM, Sigmoid Activation, Seq2Seq",Seq2Seq,LSTM,,,,
multi-level-training-and-bayesian,"Gaussian Process, HPO",Gaussian Process,,,,,
a-robust-interactive-facial-animation-editing,AutoEncoder,Autoencoder,,,,,
vae-lime-deep-generative-model-based-approach,"AutoEncoder, VAE, LIME",VAE,Autoencoder,,,,
an-autoencoder-based-approach-to-simulate,AutoEncoder,Autoencoder,,,,,
an-approach-for-auxiliary-diagnosing-and,Logistic Regression,Logistic Regression,,,,,
interpretable-multidimensional-multimodal,SVM,SVM,,,,,
bayesian-few-shot-classification-with-one-vs,"Gaussian Process, Polya-Gamma Augmentation, Softmax",Gaussian Process,,,,,
coarse-graining-molecular-dynamics-with-graph,Graph Neural Network,GNN,,,,,
machine-learning-approach-of-automatic,"KNN and IOU based verification, Leaky ReLU, Dense Connections, YOLOv1, Non Maximum Suppression, Convolution, Max Pooling, 1x1 Convolution, Dropout",YOLOv1,KNN,,,,
latent-space-time-evolution-of-non-intrusive,Gaussian Process,Gaussian Process,,,,,
a-novel-ensemble-deep-learning-model-for,"Tanh Activation, Memory Network, GRU, LSTM, Sigmoid Activation",GRU,LSTM,,,,
fedemail-performance-measurement-of-privacy,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
predicting-heave-and-surge-motions-of-a-semi,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
aarsynth-app-aware-response-synthesis-for,"Tanh Activation, LSTM, Sigmoid Activation, Seq2Seq",Seq2Seq,LSTM,,,,
machine-learning-for-complete-intersection,Linear Regression,Linear Regression,,,,,
bert-learns-and-teaches-chemistry,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Convolution, Softmax, Weight Decay",BERT,,,,,
the-price-of-tailoring-the-index-to-your-data,Linear Regression,Linear Regression,,,,,
multi-node-bert-pretraining-cost-efficient,"Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay, Linear Warmup With Cosine Annealing, Adam, Dropout, BPE, Linear Layer, Attention Dropout, Discriminative Fine-Tuning, GELU, XLNet, SentencePiece, Dense Connections, Cosine Annealing, Multi-Head Attention, GPT-2, WordPiece, Residual Connection",BERT,GPT,XLNet,,,
a-comparative-study-of-ai-based-intrusion,Q-Learning,Q-Learning,,,,,
macroeconomic-data-transformations-matter,Linear Regression,Linear Regression,,,,,
predicting-multiple-icd-10-codes-from,Logistic Regression,Logistic Regression,,,,,
to-bert-or-not-to-bert-comparing-speech-and,"Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay, Position-Wise Feed-Forward Layer, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, WordPiece, Residual Connection",BERT,Transformer,,,,
a-causal-based-framework-for-multimodal,AutoEncoder,Autoencoder,,,,,
trove-ontology-driven-weak-supervision-for,"GELU, Layer Normalization, Dense Connections, BERT, Linear Warmup, Scaled Dot-Product Attention, Softmax, Multi-Head Attention, Adam, Dropout, WordPiece, Linear Layer",BERT,,,,,
hopfield-networks-is-all-you-need,"Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay, Position-Wise Feed-Forward Layer, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, WordPiece, Residual Connection, Hopfield Layer",BERT,Transformer,,,,
unsupervised-learning-for-identifying-events,AutoEncoder,Autoencoder,,,,,
qubo-formulations-for-training-machine,SVM,SVM,,,,,
adiabatic-quantum-linear-regression,Linear Regression,Linear Regression,,,,,
learning-insulin-glucose-dynamics-in-the-wild,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
protein-structured-reservoir-computing-for,Linear Regression,Linear Regression,,,,,
boosting-ant-colony-optimization-via-solution,Logistic Regression,Logistic Regression,,,,,
a-comparison-of-synthetic-oversampling,"SMOTE, SVM",SVM,,,,,
an-automated-end-to-end-framework-for,Logistic Regression,Logistic Regression,,,,,
improving-the-performance-of-fine-grain-image,"Path Length Regularization, Leaky ReLU, Convolution, StyleGAN2, R1 Regularization, Weight Demodulation",GAN,StyleGAN2,StyleGAN,,,
an-efficient-hybrid-deep-learning-approach,"SVM, Feature Selection",SVM,,,,,
a-community-powered-search-of-machine,"Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Graph Transformer, Residual Connection, Layer Normalization, Laplacian PE, Scaled Dot-Product Attention, Softmax, LapEigen, Position-Wise Feed-Forward Layer, Adam, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
deep-learning-to-quantify-pulmonary-edema-in,AutoEncoder,Autoencoder,,,,,
mice-mining-idioms-with-contextual-embeddings,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, LSTM, Dense Connections, ELMo, BiLSTM, Multi-Head Attention, WordPiece, Residual Connection, Tanh Activation, Layer Normalization, Sigmoid Activation, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,ELMo,LSTM,BiLSTM,,
machine-learning-for-robust-identification-of,Linear Regression,Linear Regression,,,,,
prediction-of-homicides-in-urban-centers-a,Logistic Regression,Logistic Regression,,,,,
physical-action-categorization-using-signal,"k-NN, PCA, SVM",SVM,,,,,
tempnodeemb-temporal-node-embedding,Graph Neural Network,GNN,,,,,
model-patching-closing-the-subgroup,"ReLU, Cycle Consistency Loss, GAN Least Squares Loss, Batch Normalization, Residual Connection, Tanh Activation, CycleGAN, Sigmoid Activation, Leaky ReLU, PatchGAN, Instance Normalization, Residual Block, Convolution",GAN,,,,,
obtaining-adjustable-regularization-for-free,"Linear Regression, SGD",Linear Regression,,,,,
on-the-generalization-properties-of-1,Linear Regression,Linear Regression,,,,,
neural-networks-in-day-ahead-electricity,Linear Regression,Linear Regression,,,,,
predicting-future-sales-of-retail-products,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
reinforcement-learning-with-quantum,"DQN, Dense Connections, Q-Learning, Convolution, Double DQN, Double Q-learning, Experience Replay",Q-Learning,,,,,
predicting-coordinated-actuated-traffic,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
deep-learning-based-on-generative-adversarial,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
demand-forecasting-using-long-short-term,Linear Regression,Linear Regression,,,,,
name-that-manufacturer-relating-image,"Average Pooling, U-Net, Concatenated Skip Connection, Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Bottleneck Residual Block, Residual Connection, ResNet",ResNet,,,,,
slide-free-muse-microscopy-to-h-e-histology,"Cycle Consistency Loss, ReLU, GAN Least Squares Loss, Batch Normalization, Residual Connection, Tanh Activation, CycleGAN, Sigmoid Activation, Leaky ReLU, PatchGAN, Instance Normalization, Residual Block",GAN,,,,,
when-homomorphic-encryption-marries-secret,Logistic Regression,Logistic Regression,,,,,
learning-low-frequency-temporal-patterns-for,"Restricted Boltzmann Machine, AutoEncoder",Autoencoder,,,,,
duth-at-semeval-2020-task-11-bert-with-entity,"Weight Decay, Adam, Feature Selection, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax",BERT,,,,,
an-empirical-investigation-of-different,SVM,SVM,,,,,
exoplanet-validation-with-machine-learning-50,Gaussian Process,Gaussian Process,,,,,
machine-learning-approaches-to-real-estate,Logistic Regression,Logistic Regression,,,,,
fast-approximate-multi-output-gaussian,Gaussian Process,Gaussian Process,,,,,
hinglishnlp-fine-tuned-language-models-for,"Attention Dropout, GELU, DropConnect, Dense Connections, WordPiece, Residual Connection, Embedding Dropout, Layer Normalization, Sigmoid Activation, Scaled Dot-Product Attention, Softmax, RoBERTa, Weight Decay, Weight Tying, Dropout, Linear Layer, Discriminative Fine-Tuning, Variational Dropout, LSTM, Slanted Triangular Learning Rates, Multi-Head Attention, Temporal Activation Regularization, AWD-LSTM, Tanh Activation, Linear Warmup With Linear Decay, BERT, Activation Regularization, Adam, ULMFiT",BERT,LSTM,RoBERTa,,,
a-multitask-deep-learning-approach-for-user,"Scaled Dot-Product Attention, Softmax, Multi-Head Attention, Adam, Dropout, BPE, Linear Layer, Residual Connection, GELU, Layer Normalization, XLNet, SentencePiece, Linear Warmup With Linear Decay, Dense Connections",XLNet,,,,,
multimodal-learning-for-cardiovascular-risk,"Tanh Activation, LSTM, Sigmoid Activation, BiLSTM",LSTM,BiLSTM,,,,
learning-efficient-parameter-server,Q-Learning,Q-Learning,,,,,
on-transfer-learning-of-traditional-frequency,Logistic Regression,Logistic Regression,,,,,
a-new-training-protocol-for-performance,Logistic Regression,Logistic Regression,,,,,
a-novel-multiple-ensemble-learning-models,SVM,SVM,,,,,
towards-demystifying-dimensions-of-source,SVM,SVM,,,,,
knowledge-efficient-deep-learning-for-natural,"Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay, Linear Warmup With Cosine Annealing, Adam, Dropout, BPE, Linear Layer, Attention Dropout, Discriminative Fine-Tuning, Memory Network, GELU, Dense Connections, Cosine Annealing, GPT, Multi-Head Attention, WordPiece, Residual Connection",BERT,GPT,,,,
machine-learning-thermal-circuit-network,"Gaussian Process, GA",Gaussian Process,,,,,
solving-the-single-track-train-scheduling,Q-Learning,Q-Learning,,,,,
dave-deriving-automatically-verilog-from,"Scaled Dot-Product Attention, Softmax, Weight Decay, Linear Warmup With Cosine Annealing, Adam, BPE, Dropout, Linear Layer, Discriminative Fine-Tuning, Attention Dropout, GELU, Dense Connections, Cosine Annealing, Multi-Head Attention, GPT-2, Residual Connection, Layer Normalization",GPT,,,,,
quantum-long-short-term-memory,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
fairxgboost-fairness-aware-classification-in,Logistic Regression,Logistic Regression,,,,,
active-deep-learning-method-for-the-discovery,"Dense Connections, ZFNet, Local Contrast Normalization, Convolution, Softmax, Max Pooling, ReLU, Dropout",ZFNet,,,,,
black-box-to-white-box-discover-model,"Scaled Dot-Product Attention, Softmax, Weight Decay, Linear Warmup With Cosine Annealing, Adam, BPE, Dropout, Linear Layer, Discriminative Fine-Tuning, Attention Dropout, GELU, Dense Connections, Cosine Annealing, Multi-Head Attention, GPT-2, Residual Connection, Layer Normalization",GPT,,,,,
automatic-detection-of-microsleep-episodes,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
deep-learning-for-the-analysis-of-disruption,AutoEncoder,Autoencoder,,,,,
the-integrity-of-machine-learning-algorithms,Logistic Regression,Logistic Regression,,,,,
graph-convolutional-networks-reveal-neural,GCN,GCN,,,,,
molecular-insights-from-conformational,"Feedforward Network, PCA, Dense Connections, AutoEncoder, Restricted Boltzmann Machine",Autoencoder,,,,,
hierarchical-message-passing-graph-neural,Graph Neural Network,GNN,,,,,
physics-informed-gaussian-process-for-online,Gaussian Process,Gaussian Process,,,,,
method-for-classifying-a-noisy-raman-spectrum,"DCNN, k-NN, Convolution, SVM",SVM,CNN,DCNN,,,
going-deep-using-deep-learning-techniques,SVM,SVM,,,,,
health-behaviors-associated-with-the-growing,Logistic Regression,Logistic Regression,,,,,
neural-time-dependent-partial-differential,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
multi-modal-embeddings-using-multi-task,"Tanh Activation, LSTM, Sigmoid Activation, ELMo, GloVe, Softmax, BiLSTM",ELMo,LSTM,BiLSTM,,,
modern-methods-for-text-generation,"Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay, Linear Warmup With Cosine Annealing, Adam, Dropout, BPE, Linear Layer, Attention Dropout, Discriminative Fine-Tuning, GELU, Dense Connections, Cosine Annealing, Multi-Head Attention, GPT-2, WordPiece, Residual Connection",BERT,GPT,,,,
developing-and-improving-risk-models-using,Logistic Regression,Logistic Regression,,,,,
investigating-bi-lstm-and-crf-with-pos-tag,"Tanh Activation, LSTM, CRF, Sigmoid Activation, Softmax",LSTM,,,,,
fine-tuning-pre-trained-contextual-embeddings,"Attention Dropout, GELU, DropConnect, XLNet, SentencePiece, Dense Connections, WordPiece, Residual Connection, Embedding Dropout, Layer Normalization, Sigmoid Activation, Scaled Dot-Product Attention, Softmax, Weight Decay, Weight Tying, Dropout, Linear Layer, Discriminative Fine-Tuning, Variational Dropout, LSTM, Slanted Triangular Learning Rates, Multi-Head Attention, Temporal Activation Regularization, AWD-LSTM, Tanh Activation, Linear Warmup With Linear Decay, BERT, Activation Regularization, Adam, ULMFiT, BPE",BERT,LSTM,XLNet,,,
boostingbert-integrating-multi-class-boosting,"Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, RoBERTa, Weight Decay, Position-Wise Feed-Forward Layer, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer, Attention Dropout, Knowledge Distillation, GELU, Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, WordPiece, Residual Connection",BERT,Transformer,RoBERTa,,,
abstractive-information-extraction-from,"Tanh Activation, LSTM, Sigmoid Activation, BiLSTM",LSTM,BiLSTM,,,,
compressed-deep-networks-goodbye-svd-hello,"Scaled Dot-Product Attention, Softmax, RoBERTa, Weight Decay, Adam, BPE, Dropout, Linear Layer, Attention Dropout, GELU, XLNet, SentencePiece, Dense Connections, Multi-Head Attention, DistilBERT, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT",BERT,XLNet,RoBERTa,,,
fairness-constraints-in-semi-supervised,Logistic Regression,Logistic Regression,,,,,
learning-functors-using-gradient-descent,"ReLU, GAN Least Squares Loss, Cycle Consistency Loss, Batch Normalization, Residual Connection, Tanh Activation, CycleGAN, Sigmoid Activation, PatchGAN, Leaky ReLU, Instance Normalization, Convolution, Residual Block",GAN,,,,,
nextdoor-gpu-based-graph-sampling-for,"DeepWalk, GraphSAGE, node2vec",GraphSAGE,,,,,
tadgan-time-series-anomaly-detection-using,"Tanh Activation, LSTM, Sigmoid Activation, Cycle Consistency Loss",LSTM,,,,,
automated-source-code-generation-and-auto,"Layer Normalization, Dense Connections, Label Smoothing, Transformer, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, Multi-Head Attention, Absolute Position Encodings, BPE, Dropout, Linear Layer, Residual Connection",Transformer,,,,,
rcnn-for-region-of-interest-detection-in,"Convolution, Max Pooling, Softplus, ReLU, YOLOv3, Grid Sensitive, Batch Normalization, FPN, CutMix, Mish, Spatial Attention Module, Average Pooling, Cosine Annealing, Label Smoothing, Global Average Pooling, Residual Connection, Logistic Regression, Sigmoid Activation, Bottom-up Path Augmentation, k-Means Clustering, Softmax, 1x1 Convolution, Spatial Pyramid Pooling, PAFPN, DropBlock, CSPDarknet53, Darknet-53, YOLOv4, Tanh Activation",YOLOv3,Logistic Regression,T5,YOLOv4,,
m-arcsinh-an-efficient-and-reliable-function,"m-arcsinh, SVM",SVM,,,,,
interpolating-the-trace-of-the-inverse-of,Gaussian Process,Gaussian Process,,,,,
a-multimodal-memes-classification-a-survey,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
graph-representation-forecasting-of-patient-s,Graph Neural Network,GNN,,,,,
distributional-generalization-a-new-kind-of,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Bottleneck Residual Block, Residual Connection, ResNet, Distributional Generalization, Average Pooling",ResNet,,,,,
an-analysis-of-deep-neural-networks-for,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
towards-stable-imbalanced-data-classification,AutoEncoder,Autoencoder,,,,,
neu-at-wnut-2020-task-2-data-augmentation-to,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
automated-stroke-rehabilitation-assessment,Gaussian Process,Gaussian Process,,,,,
persian-ezafe-recognition-using-transformers,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
towards-computational-linguistics-in,"LSTM, Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Tanh Activation, Layer Normalization, Sigmoid Activation, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,LSTM,,,,
clevr-parser-a-graph-parser-library-for,Graph Neural Network,GNN,,,,,
hidden-incentives-for-auto-induced,Q-Learning,Q-Learning,,,,,
automatic-target-recognition-atr-from-sar,"SVM, Discrete Cosine Transform",SVM,,,,,
applying-a-random-projection-algorithm-to-1,SVM,SVM,,,,,
exploiting-vietnamese-social-media,Logistic Regression,Logistic Regression,,,,,
an-intuitive-tutorial-to-gaussian-processes,Gaussian Process,Gaussian Process,,,,,
automatic-identification-of-fossils-and,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Bottleneck Residual Block, Residual Connection, ResNet, Average Pooling",ResNet,,,,,
stock-price-prediction-using-machine-learning,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
towards-the-automation-of-a-chemical,Linear Regression,Linear Regression,,,,,
a-comparative-study-of-feature-types-for-age,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
end-to-end-prediction-of-parcel-delivery-time,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Bottleneck Residual Block, Residual Connection, ResNet, Average Pooling",ResNet,,,,,
transparency-auditability-and-explainability,Logistic Regression,Logistic Regression,,,,,
accelerating-multi-model-inference-by-merging,"Weight Decay, Adam, BPE, Dropout, Linear Layer, Attention Dropout, GELU, XLNet, SentencePiece, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax",BERT,XLNet,,,,
lateral-force-prediction-using-gaussian,Gaussian Process,Gaussian Process,,,,,
a-fast-graph-neural-network-based-method-for,Graph Neural Network,GNN,,,,,
a-vietnamese-dataset-for-evaluating-machine,"Weight Decay, Adam, Dropout, BPE, Linear Layer, Attention Dropout, GELU, Dense Connections, XLM, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax",BERT,XLM,,,,
when-will-the-mist-clear-on-the,SVM,SVM,,,,,
brain2pix-fully-convolutional-naturalistic,"Dense Connections, Convolution, Softmax, VGG, Max Pooling, ReLU, Dropout",VGG,,,,,
ac-vae-learning-semantic-representation-with,VAE,VAE,,,,,
skillbert-skilling-the-bert-to-classify,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, Spectral Clustering, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, k-Means Clustering, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
baaan-backdoor-attacks-against-auto-encoder,AutoEncoder,Autoencoder,,,,,
efficient-estimators-for-heavy-tailed-machine,Linear Regression,Linear Regression,,,,,
on-the-discovery-of-feature-importance,Logistic Regression,Logistic Regression,,,,,
improving-generalizability-of-protein,"Dense Connections, Label Smoothing, Transformer, Contrastive Learning, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, Absolute Position Encodings, Dropout, BPE, Linear Layer",Transformer,,,,,
learning-a-latent-search-space-for-routing,AutoEncoder,Autoencoder,,,,,
practical-locally-private-federated-learning,"Global Average Pooling, Convolution, Residual Block, Max Pooling, 1x1 Convolution, ReLU, Kaiming Initialization, Batch Normalization, Bottleneck Residual Block, Residual Connection, ResNet, Average Pooling",ResNet,,,,,
identifying-informative-latent-variables,GIN,GIN,,,,,
real-time-automl,Graph Neural Network,GNN,,,,,
bayesian-neural-networks-with-variance,"Tanh Activation, LSTM, Sigmoid Activation, Dropout",LSTM,,,,,
towards-practical-second-order-optimization,"Adam, Dropout, Linear Layer, Attention Dropout, Distributed Shampoo, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
a-unified-framework-for-convolution-based,"GCN, Convolution",GCN,,,,,
ae-smote-a-multi-modal-minority-oversampling,AutoEncoder,Autoencoder,,,,,
forcenet-a-graph-neural-network-for-large,Graph Neural Network,GNN,,,,,
subspace-clustering-via-robust-self,"AutoEncoder, Softmax, Spectral Clustering",Autoencoder,,,,,
entity-embedding-as-game-representation,AutoEncoder,Autoencoder,,,,,
evaluating-progress-on-machine-learning-for,Logistic Regression,Logistic Regression,,,,,
is-ai-model-interpretable-to-combat-with,"LAMB, Multi-Head Attention, Adam, WordPiece, Linear Layer, Residual Connection, ALBERT, GELU, Layer Normalization, Dense Connections, Scaled Dot-Product Attention, Softmax",BERT,ALBERT,,,,
baaan-backdoor-attacks-against-autoencoder,AutoEncoder,Autoencoder,,,,,
logan-local-group-bias-detection-by,"GAN Hinge Loss, Spectral Normalization, Dense Connections, Natural Gradient Descent, Non-Local Block, Bottleneck Residual Block, Residual Connection, Latent Optimisation, Softmax, Euclidean Norm Regularization, 1x1 Convolution, Linear Layer, Truncation Trick, Leaky ReLU, Projection Discriminator, BigGAN-deep, Feedforward Network, TTUR, LOGAN, Conditional Batch Normalization, Early Stopping, SAGAN Self-Attention Module, SAGAN, Dot-Product Attention, Convolution, Non-Local Operation, ReLU, Adam, Off-Diagonal Orthogonal Regularization, Batch Normalization",GAN,BigGAN,BigGAN-Deep,,,
improving-efficient-neural-ranking-models,"Adam, Dropout, Linear Layer, Attention Dropout, Knowledge Distillation, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
using-bayesian-deep-learning-approaches-for,"Gaussian Process, Dropout",Gaussian Process,,,,,
a-machine-learning-framework-for-les-closure,GRU,GRU,,,,,
energy-based-out-of-distribution-detection-1,"WideResNet, ReLU, Kaiming Initialization, Dropout, Batch Normalization, Residual Connection, Wide Residual Block, Average Pooling, Global Average Pooling, Softmax, Convolution",ResNet,,,,,
detecting-fine-grained-cross-lingual-semantic,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
combining-deep-learning-and-string-kernels,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
identifying-risk-of-opioid-use-disorder-for,"Tanh Activation, Logistic Regression, LSTM, Sigmoid Activation",Logistic Regression,LSTM,,,,
hypersage-generalizing-inductive-1,GraphSAGE,GraphSAGE,,,,,
epidemioptim-a-toolbox-for-the-optimization-1,Q-Learning,Q-Learning,,,,,
from-hero-to-zeroe-a-benchmark-of-low-level,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, RoBERTa, Weight Decay",BERT,RoBERTa,,,,
differentially-private-secure-multi-party,Logistic Regression,Logistic Regression,,,,,
anomaly-detection-with-conditional,"AutoEncoder, VAE",VAE,Autoencoder,,,,
online-learning-and-distributed-control-for,"Gaussian Process, Logistic Regression",Logistic Regression,Gaussian Process,,,,
category-learning-with-context-augmented,"AutoEncoder, VAE",VAE,Autoencoder,,,,
from-time-series-to-euclidean-spaces-on,AutoEncoder,Autoencoder,,,,,
model-selection-for-cross-lingual-transfer-1,mBERT,BERT,,,,,
comet-atomic-2020-on-symbolic-and-neural,"Scaled Dot-Product Attention, Softmax, Weight Decay, Linear Warmup With Cosine Annealing, Adam, BPE, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Fixed Factorized Attention, Cosine Annealing, Strided Attention, Multi-Head Attention, Residual Connection, GPT-3, Layer Normalization",GPT,,,,,
3dmolnet-a-generative-network-for-molecular,AutoEncoder,Autoencoder,,,,,
ensembles-of-convolutional-neural-networks,"Average Pooling, Dense Connections, Global Average Pooling, Concatenated Skip Connection, CheXNet, DenseNet, Convolution, Softmax, Max Pooling, 1x1 Convolution, Dense Block, ReLU, Kaiming Initialization, Dropout, Batch Normalization",CheXNet,DenseNet,,,,
a-generalizable-and-accessible-approach-to,Linear Regression,Linear Regression,,,,,
krighedge-gp-surrogates-for-delta-hedging,Gaussian Process,Gaussian Process,,,,,
binary-choice-with-asymmetric-loss-in-a-data,Logistic Regression,Logistic Regression,,,,,
a-generative-model-based-adversarial-security,AutoEncoder,Autoencoder,,,,,
generalizable-machine-learning-in,Graph Neural Network,GNN,,,,,
covid-19-and-arabic-twitter-how-can-arab,Logistic Regression,Logistic Regression,,,,,
federated-bayesian-optimization-via-thompson,Gaussian Process,Gaussian Process,,,,,
the-open-catalyst-2020-oc20-dataset-and,"Graph Neural Network, SchNet, Shifted Softplus",GNN,,,,,
concentration-of-solutions-to-random,Logistic Regression,Logistic Regression,,,,,
bert2dnn-bert-distillation-with-massive,"Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay, Position-Wise Feed-Forward Layer, Adam, Absolute Position Encodings, Dropout, BPE, Linear Layer, Attention Dropout, GELU, Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, WordPiece, Residual Connection",BERT,Transformer,,,,
connections-between-relational-event-model,"DQN, Dense Connections, Q-Learning, REM, Convolution",Q-Learning,,,,,
gender-prediction-based-on-vietnamese-names,"Tanh Activation, fastText, LSTM, Sigmoid Activation",LSTM,,,,,
multi-view-graph-contrastive-representation,"GCN, Contrastive Learning",GCN,,,,,
robustness-aware-2-bit-quantization-with-real,"Global Average Pooling, Convolution, Softmax, Max Pooling, 1x1 Convolution, Fire Module, ReLU, Dropout, Xavier Initialization, Residual Connection, SqueezeNet, Average Pooling",SqueezeNet,,,,,
sobolev-training-of-thermodynamic-informed,GRU,GRU,,,,,
transferable-graph-optimizers-for-ml,"Adaptive Softmax, Layer Normalization, Inception-v3, Scaled Dot-Product Attention, Convolution, Softmax, Transformer-XL, Max Pooling, Linear Warmup With Cosine Annealing, 1x1 Convolution, ReLU, Adam, Dropout, Linear Layer, Variational Dropout, Adaptive Input Representations, Average Pooling, Dense Connections, Cosine Annealing, Label Smoothing, WaveNet, Dilated Causal Convolution, Mixture of Logistic Distributions, Inception-v3 Module, Multi-Head Attention, Auxiliary Classifier, Residual Connection",Transformer-XL,Transformer,,,,
a-simple-spectral-failure-mode-for-graph,GCN,GCN,,,,,
ice-monitoring-in-swiss-lakes-from-optical,"DeepLabv3, Dilated Convolution, ASPP, 1x1 Convolution, SVM, Spatial Pyramid Pooling, Batch Normalization",SVM,,,,,
t-2-net-a-semi-supervised-deep-model-for,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
graphmdn-leveraging-graph-structure-and-deep,GCN,GCN,,,,,
co-embedding-of-nodes-and-edges-with-graph,"GCN, Convolution",GCN,,,,,
transgender-community-sentiment-analysis-from,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
eegsig-machine-learning-based-toolbox-for-end,SVM,SVM,,,,,
electromagnetic-source-imaging-via-a-data,"Denoising Autoencoder, AutoEncoder",Autoencoder,,,,,
strongly-incremental-constituency-parsing,Graph Neural Network,GNN,,,,,
gens-generative-encoding-networks,AutoEncoder,Autoencoder,,,,,
test-set-optimization-by-machine-learning,SVM,SVM,,,,,
accurate-prostate-cancer-detection-and,"Mask R-CNN, RPN, RoIAlign, Convolution, Softmax",CNN,,,,,
generalized-insider-attack-detection,SVM,SVM,,,,,
using-a-binary-classification-model-to,Logistic Regression,Logistic Regression,,,,,
agebo-tabular-joint-neural-architecture-and,Aging Evolution,GIN,,,,,
machine-learning-for-dynamic-resource,"Linear Regression, SVM",SVM,Linear Regression,,,,
rumor-detection-on-twitter-using-multiloss,"Tanh Activation, LSTM, Sigmoid Activation, BiLSTM",LSTM,BiLSTM,,,,
introducing-various-semantic-models-for,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, RoBERTa, Weight Decay",BERT,RoBERTa,,,,
identifying-exoplanets-with-deep-learning-iv,"Linear Regression, Gaussian Process",Linear Regression,Gaussian Process,,,,
unsupervised-intrusion-detection-system-for,AutoEncoder,Autoencoder,,,,,
using-a-bi-directional-lstm-model-with,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
machine-learning-assisted-chimera-and,SVM,SVM,,,,,
autoencoding-features-for-aviation-machine,AutoEncoder,Autoencoder,,,,,
gain-graph-attention-interaction-network-for,Graph Neural Network,GNN,,,,,
a-scalable-approach-for-privacy-preserving,Logistic Regression,Logistic Regression,,,,,
coder-knowledge-infused-cross-lingual-medical,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Contrastive Learning, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
aml-svm-adaptive-multilevel-learning-with,SVM,SVM,,,,,
polymers-for-extreme-conditions-designed,Gaussian Process,Gaussian Process,,,,,
direction-matters-on-the-implicit-1,"Linear Regression, SGD, Early Stopping",Linear Regression,,,,,
fighting-an-infodemic-covid-19-fake-news,"Logistic Regression, SVM",SVM,Logistic Regression,,,,
bangla-text-classification-using-transformers,"Layer Normalization, Dense Connections, Label Smoothing, Transformer, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Multi-Head Attention, Adam, Absolute Position Encodings, Dropout, BPE, Linear Layer, Residual Connection",Transformer,,,,,
causality-aware-counterfactual-confounding,Linear Regression,Linear Regression,,,,,
fast-and-accurate-pseudoinverse-with-sparse,Linear Regression,Linear Regression,,,,,
google-trends-analysis-of-covid-19,Linear Regression,Linear Regression,,,,,
curse-of-small-sample-size-in-forecasting-of,"Linear Regression, Feature Selection",Linear Regression,,,,,
generation-of-human-aware-navigation-maps,Graph Neural Network,GNN,,,,,
a-systematic-comparison-of-encrypted-machine,"Layer Normalization, Dense Connections, Label Smoothing, Transformer, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Multi-Head Attention, Adam, Absolute Position Encodings, Dropout, BPE, Linear Layer, Residual Connection",Transformer,,,,,
multilingual-irony-detection-with-dependency,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
energy-consumption-forecasting-using-a,Gaussian Process,Gaussian Process,,,,,
stochastic-gradient-descent-in-correlated,"Gaussian Process, SGD",Gaussian Process,,,,,
random-walk-graph-neural-networks,Graph Neural Network,GNN,,,,,
hurricane-forecasting-a-novel-multimodal,"GRU, Dense Connections, Label Smoothing, Transformer, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer",Transformer,GRU,,,,
automatic-human-sleep-stage-scoring-using,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
end-to-end-learning-from-noisy-crowd-to,SVM,SVM,,,,,
population-synthesis-for-urban-resident,cVAE,VAE,,,,,
using-graph-neural-networks-to-reconstruct,Graph Neural Network,GNN,,,,,
on-the-equivalence-of-molecular-graph-1,"GCN, Linear Layer",GCN,,,,,
learning-from-task-descriptions,"Dropout, BPE, Linear Layer, Attention Dropout, GELU, T5, Dense Connections, SentencePiece, Inverse Square Root Schedule, GLU, Multi-Head Attention, Residual Connection, Layer Normalization, Scaled Dot-Product Attention, Softmax, Adafactor",T5,,,,,
a-large-scale-database-for-graph,Graph Neural Network,GNN,,,,,
tonic-a-deep-reinforcement-learning-library,"D4PG, TD3, Target Policy Smoothing, Convolution, Weight Decay, 1x1 Convolution, Adam, ReLU, N-step Returns, A2C, Batch Normalization, Clipped Double Q-learning, PPO, SAC, Dilated Convolution, TRPO, Average Pooling, Dense Connections, Global Average Pooling, DDPG, Entropy Regularization, Experience Replay, Prioritized Experience Replay",Q-Learning,,,,,
sentiment-analysis-for-sinhala-language-using,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
assessing-wireless-sensing-potential-with,"Denoising Autoencoder, AutoEncoder",Autoencoder,,,,,
avoiding-communication-in-logistic-regression,"Logistic Regression, SGD",Logistic Regression,,,,,
machine-learning-for-phase-behavior-in-active,Graph Neural Network,GNN,,,,,
a-knowledge-distillation-ensemble-framework,"LSTM, AutoEncoder",LSTM,Autoencoder,,,,
learning-control-for-transmission-and,"Linear Layer, Tanh Activation, LSTM, Sigmoid Activation, Parrot, Scaled Dot-Product Attention, Softmax",LSTM,,,,,
machine-learning-number-fields,Logistic Regression,Logistic Regression,,,,,
a-stable-high-order-tuner-for-general-convex,Linear Regression,Linear Regression,,,,,
sentilstm-a-deep-learning-approach-for,"Tanh Activation, LSTM, Sigmoid Activation, BiLSTM",LSTM,BiLSTM,,,,
anderson-acceleration-of-coordinate-descent,Logistic Regression,Logistic Regression,,,,,
long-short-term-memory-networks-for-bandwidth,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
dual-contradistinctive-generative-autoencoder-1,"AutoEncoder, VAE",VAE,Autoencoder,,,,
pareto-efficient-acquisition-functions-for,Gaussian Process,Gaussian Process,,,,,
neural-network-gaussian-process-considering,Gaussian Process,Gaussian Process,,,,,
making-graph-neural-networks-worth-it-for-low,"Graph Neural Network, MAML",GNN,,,,,
ai-discovering-a-coordinate-system-of,"AutoEncoder, Beta-VAE",VAE,Autoencoder,,,,
solving-the-lunar-lander-problem-under,"Q-Learning, Sarsa",Q-Learning,,,,,
benchmarking-inference-performance-of-deep,"Dense Connections, Convolution, Softmax, VGG, Max Pooling, ReLU, Dropout",VGG,,,,,
self-supervised-transformers-for-activity,"Layer Normalization, Dense Connections, Label Smoothing, Transformer, Scaled Dot-Product Attention, Softmax, Position-Wise Feed-Forward Layer, Adam, Multi-Head Attention, Absolute Position Encodings, BPE, Dropout, Linear Layer, Residual Connection",Transformer,,,,,
a-robust-solution-of-a-statistical-inverse,Linear Regression,Linear Regression,,,,,
experiments-on-transfer-learning,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay",BERT,,,,,
energy-forecasting-in-smart-grid-systems-a,"Tanh Activation, LSTM, Sigmoid Activation",LSTM,,,,,
castelo-clustered-atom-subtypes-aided-lead,AutoEncoder,Autoencoder,,,,,
two-stage-transformer-model-for-covid-19-fake,"Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, Weight Decay, Position-Wise Feed-Forward Layer, Adam, Absolute Position Encodings, BPE, Dropout, Linear Layer, Attention Dropout, ALBERT, GELU, Dense Connections, Label Smoothing, Transformer, LAMB, Multi-Head Attention, WordPiece, Residual Connection",BERT,Transformer,ALBERT,,,
neural-networks-for-pulmonary-disease,DCNN,CNN,DCNN,,,,
a-novel-sentiment-analysis-engine-for,"Adam, Dropout, Linear Layer, Attention Dropout, GELU, Dense Connections, Multi-Head Attention, WordPiece, Residual Connection, Layer Normalization, Linear Warmup With Linear Decay, BERT, Scaled Dot-Product Attention, Softmax, RoBERTa, Weight Decay",BERT,RoBERTa,,,,
lockedge-low-complexity-cyberattack-detection,SVM,SVM,,,,,
self-supervised-contrastive-learning-for-2,"ResNet, Average Pooling, Dense Connections, Global Average Pooling, ColorJitter, Bottleneck Residual Block, Residual Connection, Feedforward Network, NT-Xent, Random Gaussian Blur, Residual Block, Convolution, Max Pooling, SimCLR, 1x1 Convolution, ReLU, Kaiming Initialization, Random Resized Crop, Batch Normalization",ResNet,,,,,
noma-in-uav-aided-cellular-offloading-a,"DQN, Dense Connections, Q-Learning, Convolution",Q-Learning,,,,,
a-generative-model-to-synthesize-eeg-data-for,"Leaky ReLU, DCGAN, Convolution, SVM, ReLU, Batch Normalization",SVM,GAN,DCGAN,,,
a-role-for-prior-knowledge-in-statistical,"Logistic Regression, SVM, Feature Selection",SVM,Logistic Regression,,,,
using-machine-learning-to-calibrate-storm,Logistic Regression,Logistic Regression,,,,,
fit-a-fast-and-accurate-framework-for-solving,AutoEncoder,Autoencoder,,,,,
improving-protein-gamma-turn-prediction-using,Capsule Network,Capsule Network,,,,,
capsule-deep-neural-network-for-recognition,Capsule Network,Capsule Network,,,,,
